{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cc5e77d",
   "metadata": {},
   "source": [
    "# 3. Modeling\n",
    "**Andrew Dang**  \n",
    "\n",
    "**BrainStation, Data Science**  \n",
    "\n",
    "**Previous Notebook: 2. EDA and Feature Engineering**\n",
    "\n",
    "**Next Notebook: 4. Findings**\n",
    "\n",
    "In the previous notebook, we did some initial analysis to explore the relationship between our input and target variables. \n",
    "In this notebook, we will be modeling. \n",
    "\n",
    "We are trying to predict a rating between 0 and 100, so we want to use a regression model.  \n",
    "In this notebook, I want to test the following models. \n",
    "\n",
    "1. Decision Tree Regressor\n",
    "2. KNN Regressor\n",
    "3. Ridge Regression\n",
    "4. Lasso Regression\n",
    "5. XGBoost Regression\n",
    "5. Fully connected neural networks. \n",
    "\n",
    "As we are dealing with text data, we need to represent the text data in numeric form. There are different options for this. We will explore the following options. \n",
    "\n",
    "1. Bag of Words (CountVectorizer)\n",
    "2. TF-IDF \n",
    "3. word2Vec embeddings\n",
    "4. GloVe word embeddings\n",
    "\n",
    "Another thing to consider is whether to use stemming or lemmatization. Stemming often leads to tokens with odd or incorrect spellings that are more difficult to interpret. Therefore, we want to focus our efforts on using lemmatization. \n",
    "\n",
    "We are interested in seeing what words increase/decrease the rating of a whisky. Some words are positive or negative by definition, and don't reveal much information about the whisky. Therefore, to try and get a better understanding of whisky specific language, we want to include these positive and negative words in our list of stop words (words that are ignored and do not get tokenized). We also want to do the same thing with the name of the distillers, as their appearance in a review will not tell us much about the whisky. We will also add the names of the distillers into the list of stop words. This was previosuly done in another notebook, and we will simply load them here. \n",
    "\n",
    "Now let's load in our packages. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "131ae55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data science packages\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Text data packages\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Other required packages\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Data preprocessing packages\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# modeling and metrics\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Additional NLP packages\n",
    "from sklearn.feature_extraction import text\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import plot_importance\n",
    "\n",
    "# Neural network packages\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2d750a",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bf89ea",
   "metadata": {},
   "source": [
    "## Defining functions and other utilities\n",
    "In the next few code blocks, we are doing the following:\n",
    "\n",
    "- Loading in our custom stop words that consists of positive and negative adjectives and whisky distiller names in addition to the default list of stop words.\n",
    "- Creating our lemmatizer for our text vectorizers. \n",
    "- Define a custom error and scoring function. Our scores are between 0 and 100, but there is nothing stopping our models from predicting ratings beyond these lower and upper boundaries. Our custom error function will clip all predictions below 0 to equal 0, and all predictions above 100 to 100. The custom scoring function will be passed to our models so that the predictions can be clipped during the cross-validation process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14ecc36",
   "metadata": {},
   "source": [
    "**Loading custom stop words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2354ff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open pickled custom stopwords\n",
    "filename = 'my_stop_words.pkl'\n",
    "\n",
    "infile = open(filename, 'rb')\n",
    "my_stop_words = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db16d569",
   "metadata": {},
   "source": [
    "**Creating lemmatizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fe51aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Creating lemmatizer\n",
    "# Download and instantiate the lemmatizer\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def my_lemmatizer(sentence):\n",
    "    '''\n",
    "    Takes in a string and removes punctuation, lower cases the text, and lemmatizes the text.\n",
    "    Returns a list of lemmatized words. \n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    sentence: a string\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    listoflemmad_words: list of lemmatized words. \n",
    "    '''\n",
    "    # remove punctuation and set to lower case\n",
    "    for punctuation_mark in string.punctuation:\n",
    "        sentence = sentence.replace(punctuation_mark,'').lower()\n",
    "\n",
    "    # split sentence into words\n",
    "    listofwords = sentence.split(' ')\n",
    "    listoflemmad_words = []\n",
    "    \n",
    "    # remove stopwords and any tokens that are just empty strings\n",
    "    for word in listofwords:\n",
    "        if (not word in my_stop_words and (word!='')):\n",
    "            # Stem words\n",
    "            lemmad_word = lemmatizer.lemmatize(word, pos='v')\n",
    "            listoflemmad_words.append(lemmad_word)\n",
    "\n",
    "    return listoflemmad_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38dfcaa",
   "metadata": {},
   "source": [
    "**Creating custom error and scorer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b8dbec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a custom scorer\n",
    "def clipped_mae(y_true, y_pred):\n",
    "    '''\n",
    "    A function that clips predictions above 100 to equal 100, and clip predictions under 0 to equal 0. \n",
    "    \n",
    "    Inputs:\n",
    "    -------\n",
    "    y_true: Actual label\n",
    "    y_pred: Predicted label\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    clipped_mae: The mean absolute error where predictions are clipped to remain within the boundaries of 0-100.\n",
    "    \n",
    "    '''\n",
    "    clip1 = np.where(y_pred < 0, 0, y_pred)\n",
    "    clip2 = np.where(clip1 > 100, 100, clip1)\n",
    "    clipped_mae = mean_absolute_error(y_true, clip2)\n",
    "    return clipped_mae\n",
    "\n",
    "scoring = make_scorer(clipped_mae, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684c7571",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3843643",
   "metadata": {},
   "source": [
    "## Loading fitted models\n",
    "The following function will load models that have already been fitted on the training data. This will prevent the need to refit the models every time this notebook is opened. \n",
    "\n",
    "**If you wish to see the entire fitting process, uncomment the third code block and run it.**  \n",
    "**Having the `models_loaded_flag=False` will cause each model to be re-fitted. This will take ~20-30 minutes to run.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01a7a46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fitted_models():\n",
    "    '''\n",
    "    Function that loads fitted models and sets the model_loaded_flag to True. \n",
    "    Saves readers the trouble of having to fit all the models every time they open the notebook.\n",
    "    \n",
    "    Inputs:\n",
    "    ------\n",
    "    None\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    model_dict: a dictionary that contain fitted models\n",
    "    models_loaded_flag: A boolean. If set to True, most models in the notebook will not undergo \n",
    "                        fitting, and load models from the dictionary instead.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    model_dict = {}\n",
    "    model_dict['rr_best'] = joblib.load('fitted_models/rr_best.pkl')\n",
    "    model_dict['xgb_best'] = joblib.load('fitted_models/best_xgb.pkl')\n",
    "    model_dict['gridsearch_model_bow'] = joblib.load('fitted_models/gridsearch_model_bow.pkl')\n",
    "    model_dict['gridsearch_model_tf'] = joblib.load('fitted_models/gridsearch_model_tf.pkl')\n",
    "    model_dict['gridsearch_model_xgb'] = joblib.load('fitted_models/gridsearch_model_xgb.pkl')\n",
    "    \n",
    "    models_loaded_flag = True\n",
    "    return model_dict, models_loaded_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecd49b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fitted models\n",
    "model_dict, models_loaded_flag = load_fitted_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6eef239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # UNCOMMENT THIS CODE BLOCK IF YOU WANT TO SEE THE TRAINING OF EACH MODEL. OTHERWISE, FITTED MODELS WILL BE LOADED\n",
    "# models_loaded_flag = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728c368e",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463d858e",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "**1. Using ColumnTransformer to vectorize the text data.**   \n",
    "*1.1. Instantiate a ColumnTransformer to tokenize our text data into Bag of Words representation*  \n",
    "*1.2. Instantiate a ColumnTransformer to tokenize our text data into TF-IDF representation*   \n",
    "**2. Fit a dummy regressor as a baseline model**  \n",
    "**3. Use GridSearchCV to optimize hyperparameters for Ridge, Lasso, DecisionTreeRegressor and KNNRegressor models**     \n",
    "*3.1. Fit a GridSearchCV for Bag of Words text representation*  \n",
    "*3.2. Fit a GridSearchCv for TF-IDF text representation*  \n",
    "*3.3. Optimize the best model found with GridSearch*  \n",
    "**4. Use GridSearchCV to find optimal hyperparameter values for XGBRegressor**   \n",
    "**5. Fit neural networks**  \n",
    "*5.1. Fit neural network with TF-IDF vectorzed text*  \n",
    "*5.2. Fit neural network with word2vec word embeddings*    \n",
    "*5.3. Fit neural network with GloVe word embeddings.*  \n",
    "**6. Fit our best models from steps 3 and 4 using word embeddings**  \n",
    "*6.1. Fit our best model using word2vec*  \n",
    "*6.2. Fit our best model using GloVe*  \n",
    "\n",
    "Models that perform worse than our dummy regressor will be excluded from further analysis. \n",
    "\n",
    "Now we will load in our preprocessed data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b4a2aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>review.point</th>\n",
       "      <th>Blended Malt Scotch Whisky</th>\n",
       "      <th>Blended Scotch Whisky</th>\n",
       "      <th>Grain Scotch Whisky</th>\n",
       "      <th>Single Grain Whisky</th>\n",
       "      <th>Single Malt Scotch</th>\n",
       "      <th>price_string</th>\n",
       "      <th>cleaned_reviews</th>\n",
       "      <th>review_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Johnnie Walker Blue Label, 40%</td>\n",
       "      <td>97</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>magnificently powerful and intense caramels dr...</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Black Bowmore, 1964 vintage, 42 year old, 40.5%</td>\n",
       "      <td>97</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>what impresses me most is how this whisky evol...</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bowmore 46 year old (distilled 1964), 42.9%</td>\n",
       "      <td>97</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13500.0</td>\n",
       "      <td>there have been some legendary bowmores from t...</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Compass Box The General, 53.4%</td>\n",
       "      <td>96</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>325.0</td>\n",
       "      <td>with a name inspired by a 1926 buster keaton m...</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chivas Regal Ultis, 40%</td>\n",
       "      <td>96</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>captivating enticing and wonderfully charming ...</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              name  review.point  \\\n",
       "0                   Johnnie Walker Blue Label, 40%            97   \n",
       "1  Black Bowmore, 1964 vintage, 42 year old, 40.5%            97   \n",
       "2      Bowmore 46 year old (distilled 1964), 42.9%            97   \n",
       "3                   Compass Box The General, 53.4%            96   \n",
       "4                          Chivas Regal Ultis, 40%            96   \n",
       "\n",
       "   Blended Malt Scotch Whisky  Blended Scotch Whisky  Grain Scotch Whisky  \\\n",
       "0                         0.0                    1.0                  0.0   \n",
       "1                         0.0                    0.0                  0.0   \n",
       "2                         0.0                    0.0                  0.0   \n",
       "3                         1.0                    0.0                  0.0   \n",
       "4                         1.0                    0.0                  0.0   \n",
       "\n",
       "   Single Grain Whisky  Single Malt Scotch  price_string  \\\n",
       "0                  0.0                 0.0         225.0   \n",
       "1                  0.0                 1.0        4500.0   \n",
       "2                  0.0                 1.0       13500.0   \n",
       "3                  0.0                 0.0         325.0   \n",
       "4                  0.0                 0.0         160.0   \n",
       "\n",
       "                                     cleaned_reviews  review_length  \n",
       "0  magnificently powerful and intense caramels dr...             66  \n",
       "1  what impresses me most is how this whisky evol...             82  \n",
       "2  there have been some legendary bowmores from t...             84  \n",
       "3  with a name inspired by a 1926 buster keaton m...             77  \n",
       "4  captivating enticing and wonderfully charming ...             71  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in data\n",
    "data = pd.read_pickle('data_with_engineered_feature.pkl')\n",
    "\n",
    "# Inspect the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e03b48",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d27cb1",
   "metadata": {},
   "source": [
    "## 1. Tokenizing text with ColumnTransformer\n",
    "\n",
    "### Preparing data for ColumnTransformer\n",
    "We will separate our features into text and non-text columns. This will allow us to tell the ColumnTransformer to only vectorize and transform the text data, and allow the non-text data to pass through without any transformations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "520de1b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['review.point',\n",
       " 'Blended Malt Scotch Whisky',\n",
       " 'Blended Scotch Whisky',\n",
       " 'Grain Scotch Whisky',\n",
       " 'Single Grain Whisky',\n",
       " 'Single Malt Scotch',\n",
       " 'price_string',\n",
       " 'review_length']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate our text and non-text data. 'name' needs to be dropped as ColumnTransform cannot let non-numeric data pass through.\n",
    "text_data = data['cleaned_reviews']\n",
    "non_text = data.drop(['cleaned_reviews', 'name'], axis=1)\n",
    "\n",
    "# ColumnTransformer expects a list of column names as arguments. \n",
    "non_text_cols = non_text.columns.tolist()\n",
    "non_text_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b832d7",
   "metadata": {},
   "source": [
    "Now that we have separated our data into text and non-text data, we can define how individual columns will be transformed when we use the ColumnTransformer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b890ae80",
   "metadata": {},
   "source": [
    "### 1.1. Instantiate a ColumnTransformer to tokenize our text data into Bag of Words representation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86682a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/bow_df.pkl']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform text using CountVectorizer\n",
    "# Define how individual columns in our DataFrame will be transformed\n",
    "cols_1 = [('text', CountVectorizer(stop_words=my_stop_words, min_df=5, tokenizer=my_lemmatizer), 'cleaned_reviews'),\n",
    "          ('non-text', 'passthrough', non_text_cols)]\n",
    "\n",
    "ct_1 = ColumnTransformer(cols_1, verbose_feature_names_out=False)\n",
    "\n",
    "# Fitting the ColumnTransformer\n",
    "ct_1.fit(data)\n",
    "\n",
    "# Transform data\n",
    "ct1_transformed = ct_1.transform(data)\n",
    "bow_df = pd.DataFrame(data=ct1_transformed.toarray(), columns=ct_1.get_feature_names_out())\n",
    "\n",
    "# Save data \n",
    "joblib.dump(bow_df, 'data/bow_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f96c560",
   "metadata": {},
   "source": [
    "### 1.2. Instantiate a ColumnTransformer to tokenize our text data into TF-IDF representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bce43604",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/tfidf_df.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform text using TfidfVectorizer\n",
    "# Define how individual columns in our DataFrame will be transformed, only want to scale price_string, and review_length\n",
    "cols_2 = [('t', TfidfVectorizer(stop_words=my_stop_words, min_df=5, tokenizer=my_lemmatizer), 'cleaned_reviews'),\n",
    "          ('nt', 'passthrough', non_text_cols)]\n",
    "\n",
    "ct_2 = ColumnTransformer(cols_2, verbose_feature_names_out=False)\n",
    "\n",
    "# Fit\n",
    "ct_2.fit(data)\n",
    "\n",
    "# transform data\n",
    "ct2_transformed = ct_2.transform(data)\n",
    "tfidf_df = pd.DataFrame(data=ct2_transformed.toarray(), columns=ct_2.get_feature_names_out())\n",
    "\n",
    "# Save data\n",
    "joblib.dump(tfidf_df, 'data/tfidf_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa7eeaf",
   "metadata": {},
   "source": [
    "## min_df \n",
    "min_df discards tokens that don't show up in at least this number of documents. In previous notebooks, it was determined that having min_df=5 gave us a manageable amount of tokens, just over 2,000. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efaee44",
   "metadata": {},
   "source": [
    "Now that we have transformed our data, let's inspect each DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c26d1834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>120</th>\n",
       "      <th>12000</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>...</th>\n",
       "      <th>ìle</th>\n",
       "      <th>—</th>\n",
       "      <th>review.point</th>\n",
       "      <th>Blended Malt Scotch Whisky</th>\n",
       "      <th>Blended Scotch Whisky</th>\n",
       "      <th>Grain Scotch Whisky</th>\n",
       "      <th>Single Grain Whisky</th>\n",
       "      <th>Single Malt Scotch</th>\n",
       "      <th>price_string</th>\n",
       "      <th>review_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13500.0</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>325.0</td>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>71.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2190 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     1   10  100  1000   11   12  120  12000   13   14  ...  ìle    —  \\\n",
       "0  0.0  0.0  0.0   0.0  0.0  0.0  0.0    0.0  0.0  0.0  ...  0.0  0.0   \n",
       "1  0.0  0.0  0.0   0.0  0.0  0.0  0.0    0.0  0.0  0.0  ...  0.0  0.0   \n",
       "2  0.0  0.0  0.0   0.0  0.0  0.0  0.0    0.0  0.0  0.0  ...  0.0  0.0   \n",
       "3  0.0  0.0  0.0   0.0  0.0  0.0  0.0    0.0  0.0  0.0  ...  0.0  0.0   \n",
       "4  0.0  0.0  0.0   0.0  0.0  0.0  0.0    0.0  0.0  0.0  ...  0.0  0.0   \n",
       "\n",
       "   review.point  Blended Malt Scotch Whisky  Blended Scotch Whisky  \\\n",
       "0          97.0                         0.0                    1.0   \n",
       "1          97.0                         0.0                    0.0   \n",
       "2          97.0                         0.0                    0.0   \n",
       "3          96.0                         1.0                    0.0   \n",
       "4          96.0                         1.0                    0.0   \n",
       "\n",
       "   Grain Scotch Whisky  Single Grain Whisky  Single Malt Scotch  price_string  \\\n",
       "0                  0.0                  0.0                 0.0         225.0   \n",
       "1                  0.0                  0.0                 1.0        4500.0   \n",
       "2                  0.0                  0.0                 1.0       13500.0   \n",
       "3                  0.0                  0.0                 0.0         325.0   \n",
       "4                  0.0                  0.0                 0.0         160.0   \n",
       "\n",
       "   review_length  \n",
       "0           66.0  \n",
       "1           82.0  \n",
       "2           84.0  \n",
       "3           77.0  \n",
       "4           71.0  \n",
       "\n",
       "[5 rows x 2190 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect Bag of Words DataFrame\n",
    "bow_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06d64e45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>120</th>\n",
       "      <th>12000</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>...</th>\n",
       "      <th>ìle</th>\n",
       "      <th>—</th>\n",
       "      <th>review.point</th>\n",
       "      <th>Blended Malt Scotch Whisky</th>\n",
       "      <th>Blended Scotch Whisky</th>\n",
       "      <th>Grain Scotch Whisky</th>\n",
       "      <th>Single Grain Whisky</th>\n",
       "      <th>Single Malt Scotch</th>\n",
       "      <th>price_string</th>\n",
       "      <th>review_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13500.0</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>325.0</td>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>71.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2190 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     1   10  100  1000   11   12  120  12000   13   14  ...  ìle    —  \\\n",
       "0  0.0  0.0  0.0   0.0  0.0  0.0  0.0    0.0  0.0  0.0  ...  0.0  0.0   \n",
       "1  0.0  0.0  0.0   0.0  0.0  0.0  0.0    0.0  0.0  0.0  ...  0.0  0.0   \n",
       "2  0.0  0.0  0.0   0.0  0.0  0.0  0.0    0.0  0.0  0.0  ...  0.0  0.0   \n",
       "3  0.0  0.0  0.0   0.0  0.0  0.0  0.0    0.0  0.0  0.0  ...  0.0  0.0   \n",
       "4  0.0  0.0  0.0   0.0  0.0  0.0  0.0    0.0  0.0  0.0  ...  0.0  0.0   \n",
       "\n",
       "   review.point  Blended Malt Scotch Whisky  Blended Scotch Whisky  \\\n",
       "0          97.0                         0.0                    1.0   \n",
       "1          97.0                         0.0                    0.0   \n",
       "2          97.0                         0.0                    0.0   \n",
       "3          96.0                         1.0                    0.0   \n",
       "4          96.0                         1.0                    0.0   \n",
       "\n",
       "   Grain Scotch Whisky  Single Grain Whisky  Single Malt Scotch  price_string  \\\n",
       "0                  0.0                  0.0                 0.0         225.0   \n",
       "1                  0.0                  0.0                 1.0        4500.0   \n",
       "2                  0.0                  0.0                 1.0       13500.0   \n",
       "3                  0.0                  0.0                 0.0         325.0   \n",
       "4                  0.0                  0.0                 0.0         160.0   \n",
       "\n",
       "   review_length  \n",
       "0           66.0  \n",
       "1           82.0  \n",
       "2           84.0  \n",
       "3           77.0  \n",
       "4           71.0  \n",
       "\n",
       "[5 rows x 2190 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect TF-IDF DataFrame\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51994d03",
   "metadata": {},
   "source": [
    "### Splitting the data\n",
    "Now that we have our text data tokenized, we can split our data into training and test sets. We will do this for our Bag of Words tokenized words annd TF-IDF tokenized words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3352353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data\n",
    "X_bow = bow_df.drop('review.point', axis=1)\n",
    "y_bow = bow_df['review.point']\n",
    "\n",
    "X_tfidf = tfidf_df.drop('review.point', axis=1)\n",
    "y_tfidf = tfidf_df['review.point']\n",
    "\n",
    "# train_test_split\n",
    "X_train_b, X_test_b, y_train_b, y_test_b = train_test_split(X_bow, y_bow, train_size=0.8, random_state=88)\n",
    "X_train_t, X_test_t, y_train_t, y_test_t = train_test_split(X_tfidf, y_tfidf, train_size=0.8, random_state=88)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29ef8d9",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691891c9",
   "metadata": {},
   "source": [
    "## 2. Baseline model\n",
    "We will use a model that always predicts the mean rating as a baseline model. Any model that performs worse than the baseline will not undergo further analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "532cb74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The baseline model that always predicts the mean has an MAE of: 3.1338969052732875\n"
     ]
    }
   ],
   "source": [
    "# Dummy regressor - Baseline model \n",
    "baseline = DummyRegressor(strategy='mean')\n",
    "baseline.fit(X_train_t, y_train_t)\n",
    "\n",
    "# make predictions\n",
    "y_pred = baseline.predict(X_test_t)\n",
    "\n",
    "# use MAE to determine baseline score \n",
    "dummy_mae = clipped_mae(y_test_t, y_pred) \n",
    "\n",
    "print(f'The baseline model that always predicts the mean has an MAE of: {dummy_mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6802bb1d",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e179123",
   "metadata": {},
   "source": [
    "## 3. GridSearchCV: Finding optimal hyperparameter values, and the model with the lowest error. \n",
    "In the next two code blocks, we will use GridSearchCV to tune hyperparameters for 4 types of models - Ridge regression, Lasso regression, Decision Tree Regressor, and KNN Regressor. Once the grid search is complete, we will print out the best estimator, which will tell us which of these models had the lowest cross validation score, which would suggest it has the lowest error, as well as what the hyperparameters were to achieve this cross validation score. \n",
    "\n",
    "A grid search was fitted for each of our two text representations - Bag of Words, and TF-IDF. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0e774c",
   "metadata": {},
   "source": [
    "### 3.1. Fitting GridSearchCV for our Bag of Words text representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e374f963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaler', MinMaxScaler()), ('model', Ridge(alpha=10))])\n"
     ]
    }
   ],
   "source": [
    "# If models are loaded, extract the fitted model from the models dictionary. If they are not loaded, train the model. \n",
    "if models_loaded_flag:\n",
    "    gridsearch_model_bow = model_dict['gridsearch_model_bow']\n",
    "    \n",
    "    # Print the best estimator\n",
    "    print(gridsearch_model_bow.best_estimator_)\n",
    "\n",
    "else:\n",
    "    estimators = [('scaler', StandardScaler()),\n",
    "                  ('model', Ridge())]\n",
    "\n",
    "    bow_pipe = Pipeline(estimators)\n",
    "\n",
    "    param_grid = [\n",
    "                {'model': [Ridge()],\n",
    "                 'scaler': [StandardScaler(), MinMaxScaler()],\n",
    "                 'model__alpha': [1,10,100]},\n",
    "                {'model': [Lasso()],\n",
    "                 'scaler': [StandardScaler(), MinMaxScaler()],\n",
    "                 'model__alpha': [1,10,100]},\n",
    "                {'model': [KNeighborsRegressor()],\n",
    "                 'scaler': [StandardScaler(), MinMaxScaler()],\n",
    "                 'model__n_neighbors': [3,5,7]},\n",
    "                {'model': [DecisionTreeRegressor()],\n",
    "                 'scaler': [None],\n",
    "                 'model__max_depth': [4,5,6],\n",
    "                 'model__min_samples_leaf': [2,3,4]}\n",
    "    ]\n",
    "\n",
    "    bow_grid = GridSearchCV(bow_pipe, param_grid, cv=5, scoring=scoring, n_jobs=-1)\n",
    "    fittedgrid_bow = bow_grid.fit(X_train_b, y_train_b)\n",
    "\n",
    "    # best estimator\n",
    "    print(fittedgrid_bow.best_estimator_)\n",
    "    \n",
    "    # save best model_tf as pickle\n",
    "    gridsearch_model_bow = fittedgrid_bow\n",
    "    joblib.dump(gridsearch_model_bow, 'fitted_models/gridsearch_model_bow.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42edf2e4",
   "metadata": {},
   "source": [
    "### 3.2. Fitting GridSearchCV for our TF-IDF text representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea1843a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaler', MinMaxScaler()), ('model', Ridge(alpha=10))])\n"
     ]
    }
   ],
   "source": [
    "# If models are loaded, extract the fitted model from the models dictionary. If they are not loaded, train the model. \n",
    "if models_loaded_flag:\n",
    "    gridsearch_model_tf = model_dict['gridsearch_model_tf']\n",
    "\n",
    "    # Print the best estimator\n",
    "    print(gridsearch_model_tf.best_estimator_)\n",
    "    \n",
    "else:\n",
    "    estimators = [('scaler', StandardScaler()),\n",
    "                  ('model', Ridge())]\n",
    "\n",
    "    tf_pipe = Pipeline(estimators)\n",
    "\n",
    "    param_grid = [\n",
    "                {'model': [Ridge()],\n",
    "                 'scaler': [StandardScaler(), MinMaxScaler()],\n",
    "                 'model__alpha': [1,10,100]},\n",
    "                {'model': [Lasso()],\n",
    "                 'scaler': [StandardScaler(), MinMaxScaler()],\n",
    "                 'model__alpha': [1,10,100]},\n",
    "                {'model': [KNeighborsRegressor()],\n",
    "                 'scaler': [StandardScaler(), MinMaxScaler()],\n",
    "                 'model__n_neighbors': [3,5,7]},\n",
    "                {'model': [DecisionTreeRegressor()],\n",
    "                 'scaler': [None],\n",
    "                 'model__max_depth': [4,5,6],\n",
    "                 'model__min_samples_leaf': [2,3,4]}\n",
    "    ]\n",
    "\n",
    "    tf_grid = GridSearchCV(tf_pipe, param_grid, cv=5, scoring=scoring, n_jobs=-1)\n",
    "    fittedgrid_tf = tf_grid.fit(X_train_t, y_train_t)\n",
    "\n",
    "    # best estimator\n",
    "    print(fittedgrid_tf.best_estimator_)\n",
    "    \n",
    "    # save best model_tf as pickle\n",
    "    gridsearch_model_tf = fittedgrid_tf\n",
    "    joblib.dump(gridsearch_model_tf, 'fitted_models/gridsearch_model_tf.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6159b9ca",
   "metadata": {},
   "source": [
    "Running the above two GridSearchCV's showed us that in both text representations, the Ridge regression model gave us the best results. We will exclude the other models from further investigation. Let's see how the different text representations perform on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c561c949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The TF-IDF model has a test set MAE of 5.518132534456723\n",
      "The Bag of Words model has a test set MAE of 3.059484108866949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\anaconda3\\envs\\capstone\\lib\\site-packages\\sklearn\\base.py:446: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\andre\\anaconda3\\envs\\capstone\\lib\\site-packages\\sklearn\\base.py:446: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n"
     ]
    }
   ],
   "source": [
    "# Scale data before testings\n",
    "# scale bow data first\n",
    "scaler1 = MinMaxScaler()\n",
    "scaler1.fit(X_train_b)\n",
    "\n",
    "# Transform\n",
    "X_train_scaled_b = scaler1.transform(X_train_b)\n",
    "X_test_scaled_b = scaler1.transform(X_test_b)\n",
    "\n",
    "# scale tfidf data next\n",
    "scaler2 = MinMaxScaler()\n",
    "scaler2.fit(X_train_t)\n",
    "\n",
    "# Transform\n",
    "X_train_scaled_t = scaler2.transform(X_train_t)\n",
    "X_test_scaled_t = scaler2.transform(X_test_t)\n",
    "\n",
    "# Predicting best bow model\n",
    "bow_preds = gridsearch_model_bow.predict(X_test_scaled_b)\n",
    "bow_clipped_mae = clipped_mae(y_test_b, bow_preds)\n",
    "\n",
    "# Predicting with best TF-IDF model \n",
    "tf_preds = gridsearch_model_tf.predict(X_test_scaled_t)\n",
    "tf_clipped_mae = clipped_mae(y_test_t, tf_preds)\n",
    "\n",
    "print(f\"The TF-IDF model has a test set MAE of {tf_clipped_mae}\")\n",
    "print(f\"The Bag of Words model has a test set MAE of {bow_clipped_mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62ee160",
   "metadata": {},
   "source": [
    "## 3.3. Further optimize the best model found with GridSearchCV\n",
    "It appears that we are getting the best results from our Ridge regression model using TF-IDF vectorized text, and scaling our data with a MinMaxScaler. We are going to see if we can optimize this model even further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e06564c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAE of the test set is 2.629644529107421\n"
     ]
    }
   ],
   "source": [
    "# If models are loaded, extract the fitted model from the models dictionary. If they are not loaded, train the model. \n",
    "if models_loaded_flag:\n",
    "    # load model from dictionary\n",
    "    rr_best = model_dict['rr_best']\n",
    "    \n",
    "    # make predictions on our fitted model\n",
    "    y_pred = rr_best.predict(X_test_scaled_b)\n",
    "    \n",
    "    # calculate MAE on test set\n",
    "    best_rr_clipped_mae = clipped_mae(y_test_b, y_pred)\n",
    "    \n",
    "    print(f'The MAE of the test set is {best_rr_clipped_mae}')\n",
    "\n",
    "else: \n",
    "\n",
    "    # List of alpha values we want to test; our best model from GridSearchCV had alpha=10 so we will start testing values from\n",
    "    # there\n",
    "    reg_term_list = list(range(10,31))\n",
    "\n",
    "    cv_scores = []\n",
    "    best_score = np.inf\n",
    "    best_alpha = []\n",
    "\n",
    "    for alpha in reg_term_list: \n",
    "        rr = Ridge(alpha=alpha)\n",
    "        cv_score = np.mean(cross_val_score(rr, X_train_scaled_b, y_train_b, cv=5, scoring=scoring))*-1\n",
    "    \n",
    "        cv_scores.append(cv_score)\n",
    "    \n",
    "        if cv_score < best_score:\n",
    "            best_score = cv_score\n",
    "            best_alpha = alpha\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(reg_term_list, cv_scores, color='red', label = 'Cross Validation Score')\n",
    "    plt.title('MAE for different values of alpha')\n",
    "    plt.xlabel('Alpha value')\n",
    "    plt.ylabel('Mean Absolute Error')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(f'The lowest MAE is {best_score} with a regularization term of {best_alpha}') \n",
    "\n",
    "    # recreate best model and test against test set\n",
    "    rr_best = Ridge(alpha=best_alpha)\n",
    "    rr_best.fit(X_train_scaled_b, y_train_b) \n",
    "\n",
    "    y_pred = rr_best.predict(X_test_scaled_b)\n",
    "    best_rr_clipped_mae = clipped_mae(y_test_b, y_pred)\n",
    "\n",
    "    print(f'The MAE of the test set is {best_rr_clipped_mae}')\n",
    "\n",
    "    # pickle our best model \n",
    "    joblib.dump(rr_best, 'fitted_models/rr_best.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e0c29e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAE of the test set is 2.581413763814446\n"
     ]
    }
   ],
   "source": [
    "# If models are loaded, extract the fitted model from the models dictionary. If they are not loaded, train the model. \n",
    "if models_loaded_flag:\n",
    "    # load model from dictionary\n",
    "    rr_best = model_dict['rr_best']\n",
    "    \n",
    "    # make predictions on our fitted model\n",
    "    y_pred = rr_best.predict(X_test_scaled_t)\n",
    "    \n",
    "    # calculate MAE on test set\n",
    "    best_rr_clipped_mae = clipped_mae(y_test_t, y_pred)\n",
    "    \n",
    "    print(f'The MAE of the test set is {best_rr_clipped_mae}')\n",
    "\n",
    "else: \n",
    "\n",
    "    # List of alpha values we want to test; our best model from GridSearchCV had alpha=10 so we will start testing values from\n",
    "    # there\n",
    "    reg_term_list = list(range(10,31))\n",
    "\n",
    "    cv_scores = []\n",
    "    best_score = np.inf\n",
    "    best_alpha = []\n",
    "\n",
    "    for alpha in reg_term_list: \n",
    "        rr = Ridge(alpha=alpha)\n",
    "        cv_score = np.mean(cross_val_score(rr, X_train_scaled_t, y_train_b, cv=5, scoring=scoring))*-1\n",
    "    \n",
    "        cv_scores.append(cv_score)\n",
    "    \n",
    "        if cv_score < best_score:\n",
    "            best_score = cv_score\n",
    "            best_alpha = alpha\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(reg_term_list, cv_scores, color='red', label = 'Cross Validation Score')\n",
    "    plt.title('MAE for different values of alpha')\n",
    "    plt.xlabel('Alpha value')\n",
    "    plt.ylabel('Mean Absolute Error')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(f'The lowest MAE is {best_score} with a regularization term of {best_alpha}') \n",
    "\n",
    "    # recreate best model and test against test set\n",
    "    rr_best = Ridge(alpha=best_alpha)\n",
    "    rr_best.fit(X_train_scaled_t, y_train_t) \n",
    "\n",
    "    y_pred = rr_best.predict(X_test_scaled_t)\n",
    "    best_rr_clipped_mae = clipped_mae(y_test_t, y_pred)\n",
    "\n",
    "    print(f'The MAE of the test set is {best_rr_clipped_mae}')\n",
    "\n",
    "    # pickle our best model \n",
    "    joblib.dump(rr_best, 'fitted_models/rr_best.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d85c93b",
   "metadata": {},
   "source": [
    "### Best model so far: Ridge regression with TF-IDF\n",
    "So far we have used GridSearch to rule out 3 different models. We found that Ridge regression had the best cross validation score. When we compared TF-IDF tokenization against Bag of Words tokenization, Bag of Words had a lower mean absolute error on the test set (3.05). We tried to optimize the Ridge regression model by testing more values for the regularization term, and found that the best model used TF-IDF representation (MAE = 2.581). The Dummy regressor had a mean absoute error of 3.133, so the Ridge regression model is our candidate for best model so far. \n",
    "\n",
    "The next step is to investigate ensemble methods and neural networks. \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457134d6",
   "metadata": {},
   "source": [
    "## 4. XGBRegressor \n",
    "We are interested in the difference in language used between high scoring whiskys, and lower scoring whiskys. The manner in which we do this will be to investigate the coefficient of each word in our vocabulary. While ensemble methods and neural networks may provide a more accurate model, we lose ease of interpretability with these models. \n",
    "\n",
    "With that in mind, we will fit an XGBRegressor model and try fitting different neural networks and see how accurate our predictions can get.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fa1c48",
   "metadata": {},
   "source": [
    "In the code block below, we will use GridSearchCV to help us optimze the hyperparameters for the XGBRegressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89a3d93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('model',\n",
      "                 XGBRegressor(base_score=0.5, booster='gbtree',\n",
      "                              colsample_bylevel=1, colsample_bynode=1,\n",
      "                              colsample_bytree=1, enable_categorical=False,\n",
      "                              gamma=0, gpu_id=-1, importance_type=None,\n",
      "                              interaction_constraints='', learning_rate=0.3,\n",
      "                              max_delta_step=0, max_depth=1, min_child_weight=1,\n",
      "                              missing=nan, monotone_constraints='()',\n",
      "                              n_estimators=400, n_jobs=8, num_parallel_tree=1,\n",
      "                              predictor='auto', random_state=0, reg_alpha=0,\n",
      "                              reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "                              tree_method='exact', validate_parameters=1,\n",
      "                              verbosity=None))])\n",
      "\n",
      "The MAE on the test set is 2.485620715304844\n"
     ]
    }
   ],
   "source": [
    "# If models are loaded, extract the fitted model from the models dictionary. If they are not loaded, optimize the hyperparameters. \n",
    "if models_loaded_flag:\n",
    "    gridsearch_model_xgb = model_dict['gridsearch_model_xgb']\n",
    "    \n",
    "    # Print the best estimator\n",
    "    print(gridsearch_model_xgb.best_estimator_)\n",
    "    \n",
    "    # Test MAE of the best model \n",
    "    predictions = gridsearch_model_xgb.predict(X_test_t)\n",
    "    print(f'\\nThe MAE on the test set is {clipped_mae(y_test_t, predictions)}')\n",
    "    \n",
    "else:    \n",
    "\n",
    "    # Using GridSearchCV to find the optimal values for XGBRegressor hyperparameters\n",
    "    # Setup the pipeline\n",
    "    estimators = [('model', XGBRegressor())]\n",
    "    pipe = Pipeline(estimators)\n",
    "\n",
    "    # Define which hyperparameters we want to tune\n",
    "    params = {'model__max_depth': [1, 2, 3, 4],\n",
    "              'model__learning_rate': [0.01, 0.1, 0.3],\n",
    "              'model__n_estimators': [200, 400, 600]}\n",
    "\n",
    "    # Fit GridSearchCV\n",
    "    grid_search = GridSearchCV(pipe, param_grid=params, scoring=scoring, n_jobs=-1)\n",
    "    fitted_search = grid_search.fit(X_train_t, y_train_t)\n",
    "    \n",
    "    # Print the optial hyperparameter values \n",
    "    print(fitted_search.best_estimator_)\n",
    "    \n",
    "    # Test MAE of the best model \n",
    "    predictions = fitted_search.predict(X_test_t)\n",
    "    print(f'\\nThe MAE on the test set is {clipped_mae(y_test_t, predictions)}')\n",
    "\n",
    "    # Save to pickle\n",
    "    gridsearch_model_xgb = fitted_search\n",
    "    joblib.dump(gridsearch_model_xgb, 'gridsearch_model_xgb.pkl')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b5eb42",
   "metadata": {},
   "source": [
    "The XGBRegressor model has a better test mean absolute error than our Ridge regression model. We will fit a new XGBRegressor with the optimized hyperparameter values and save the fitted model so we can further investigate the model later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6e13a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test set MAE was 2.491102035964252\n"
     ]
    }
   ],
   "source": [
    "# If models are loaded, extract the fitted model from the models dictionary. If they are not loaded, fit the model. \n",
    "if models_loaded_flag:\n",
    "    my_XGB = model_dict['xgb_best']\n",
    "    \n",
    "    # Make predictions\n",
    "    y_preds = my_XGB.predict(X_test_t)\n",
    "    my_XGB_clipped_mae = clipped_mae(y_test_t, y_preds)\n",
    "\n",
    "    # Print the MAE of the test set\n",
    "    print(f'The test set MAE was {my_XGB_clipped_mae}')\n",
    "\n",
    "else:\n",
    "    # Recreate the best model so we can look at feature importance\n",
    "    my_XGB = XGBRegressor(n_estimators=400, learning_rate=0.3, max_depth=1)\n",
    "\n",
    "    # Fit model\n",
    "    my_XGB.fit(X_train_t, y_train_t)\n",
    "\n",
    "    # Make predictions\n",
    "    y_preds = my_XGB.predict(X_test_t)\n",
    "    my_XGB_clipped_mae = clipped_mae(y_test_t, y_preds)\n",
    "\n",
    "    # Print the MAE of the test set\n",
    "    print(f'The test set MAE was {my_XGB_clipped_mae}')\n",
    "\n",
    "    # Save best fitted XGB as pickle\n",
    "    joblib.dump(my_XGB, 'fitted_models/best_xgb.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be24970",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8049db9",
   "metadata": {},
   "source": [
    "## 5. Fit Neural Networks\n",
    "We will fit neural networks with the following text representations:\n",
    "1. TF-IDF\n",
    "2. word2vec word embeddings\n",
    "3. Global Vectors for Word Representation (GloVe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259be061",
   "metadata": {},
   "source": [
    "### 5.1. Neural network 1: TF-IDF\n",
    "In the next few code blocks, we are compiling, training, and evaluating a neural network that is using TF-IDF tokenized text as inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0b3d2e",
   "metadata": {},
   "source": [
    "*Compiling the model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76b94893",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Declare the hidden layers\n",
    "model.add(layers.Dense(128, activation=\"relu\"))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(128, activation=\"relu\"))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(128, activation=\"relu\"))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(128, activation=\"relu\"))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(128, activation=\"relu\"))\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "    # Declare the output layer\n",
    "model.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "    # Compile\n",
    "model.compile(\n",
    "        # Optimizer\n",
    "    optimizer=keras.optimizers.Adam(),  \n",
    "        # Loss function to minimize\n",
    "    loss=keras.losses.MeanAbsoluteError()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e8d45c",
   "metadata": {},
   "source": [
    "*Training the model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35c10b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "45/45 [==============================] - 1s 7ms/step - loss: 51.9995 - val_loss: 11.0355\n",
      "Epoch 2/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 12.9272 - val_loss: 8.6032\n",
      "Epoch 3/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 12.2265 - val_loss: 10.2987\n",
      "Epoch 4/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 11.7200 - val_loss: 8.2000\n",
      "Epoch 5/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 11.2365 - val_loss: 6.7558\n",
      "Epoch 6/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 11.3265 - val_loss: 6.4865\n",
      "Epoch 7/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 10.5121 - val_loss: 6.3811\n",
      "Epoch 8/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 9.8488 - val_loss: 5.8461\n",
      "Epoch 9/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 9.9899 - val_loss: 6.7266\n",
      "Epoch 10/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 9.4701 - val_loss: 6.5643\n",
      "Epoch 11/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 9.2544 - val_loss: 8.1695\n",
      "Epoch 12/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 9.0640 - val_loss: 5.8681\n",
      "Epoch 13/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 8.6067 - val_loss: 8.9524\n",
      "Epoch 14/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 8.7867 - val_loss: 5.6516\n",
      "Epoch 15/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 8.7305 - val_loss: 13.4849\n",
      "Epoch 16/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 8.3949 - val_loss: 10.1676\n",
      "Epoch 17/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 8.3091 - val_loss: 9.0079\n",
      "Epoch 18/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 8.8580 - val_loss: 10.0958\n",
      "Epoch 19/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 8.1735 - val_loss: 7.6935\n",
      "Epoch 20/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 7.8407 - val_loss: 4.5334\n",
      "Epoch 21/40\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 7.9301 - val_loss: 4.0292\n",
      "Epoch 22/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 7.5416 - val_loss: 5.4639\n",
      "Epoch 23/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 7.3700 - val_loss: 3.9586\n",
      "Epoch 24/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 7.6636 - val_loss: 5.8364\n",
      "Epoch 25/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 7.4050 - val_loss: 7.0993\n",
      "Epoch 26/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 7.1656 - val_loss: 3.4542\n",
      "Epoch 27/40\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 7.5868 - val_loss: 3.7894\n",
      "Epoch 28/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 7.2089 - val_loss: 5.7490\n",
      "Epoch 29/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 7.1647 - val_loss: 6.0167\n",
      "Epoch 30/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 7.0967 - val_loss: 6.1385\n",
      "Epoch 31/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 6.9108 - val_loss: 5.8244\n",
      "Epoch 32/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 7.0193 - val_loss: 4.4334\n",
      "Epoch 33/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 7.3656 - val_loss: 3.2218\n",
      "Epoch 34/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 6.6866 - val_loss: 7.9399\n",
      "Epoch 35/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 6.8452 - val_loss: 4.3850\n",
      "Epoch 36/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 6.8351 - val_loss: 4.0939\n",
      "Epoch 37/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 6.4118 - val_loss: 3.3514\n",
      "Epoch 38/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 6.4087 - val_loss: 5.0706\n",
      "Epoch 39/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 6.7133 - val_loss: 6.2161\n",
      "Epoch 40/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 6.4921 - val_loss: 3.2127\n"
     ]
    }
   ],
   "source": [
    "# Stop training early if validation loss doesn't go down \n",
    "early_stop = EarlyStopping(monitor='val_loss', \n",
    "                       patience=10, \n",
    "                       mode='min', \n",
    "                       verbose=1,\n",
    "                       restore_best_weights=True)\n",
    "\n",
    "# Train\n",
    "history = model.fit(X_train_scaled_t,\n",
    "                    y_train_t, \n",
    "                    epochs=40, \n",
    "                    verbose=1, \n",
    "                    validation_split=0.2, \n",
    "                    callbacks=[early_stop])\n",
    "\n",
    "# Save model \n",
    "tf.keras.models.save_model(model, 'fitted_models/nn_tf.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120646e5",
   "metadata": {},
   "source": [
    "*Model evaluation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12c6a72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MAE: 6.4921\n",
      "Test MAE: 2.9905\n",
      "\n",
      "The clipped MAE on the test set is 2.9905431530788906\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the network\n",
    "train_loss = history.history[\"loss\"][-1]\n",
    "result = model.evaluate(X_test_scaled_t, y_test_t, verbose=0)\n",
    "\n",
    "print(f\"Train MAE: {train_loss:.4f}\")\n",
    "print(f\"Test MAE: {result:.4f}\") \n",
    "\n",
    "# Generate predictions\n",
    "predictions = model.predict(X_test_scaled_t)\n",
    "\n",
    "# Score our predictions based on our custom clipped MAE\n",
    "print(f'\\nThe clipped MAE on the test set is {clipped_mae(y_test_t, predictions)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516529b3",
   "metadata": {},
   "source": [
    "This neural network is performing worse than both the dummy regressor, and the current best model, Ridge regression (MAE = 2.58). We will exclude this model from further investigation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6b51e2",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d411d9fe",
   "metadata": {},
   "source": [
    "## 5.2. Neural network 2: word2vec\n",
    "The second neural network will use word2vec word embeddings as inputs. We will load in the word embeddings, and create a function that will calculate the vectorized representation of each review. Then, we will convert each review to their vectorized representation, and compile, train and evaluate the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b25e647",
   "metadata": {},
   "source": [
    "First, we will have to load the word embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24112ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load word2vec word embeddings\n",
    "w2v = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    'lexvec.enwiki+newscrawl.300d.W.pos.vectors', binary=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571642dc",
   "metadata": {},
   "source": [
    "Just use text data when using word embeddings. Let's use the original, unprocessed data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b5f31315",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = pd.read_csv('scotch_review.csv')\n",
    "X = original_data['description']\n",
    "y = original_data['review.point']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f225196",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=88)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12650ad8",
   "metadata": {},
   "source": [
    "Define a function that takes the average word embedding for the entire review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8685b758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2vec(text, embedding):\n",
    "    \"\"\"\n",
    "    Embed a sentence by averaging the word vectors of the tokenized text. Out-of-vocabulary words are replaced by the zero-vector.\n",
    "    -----\n",
    "    \n",
    "    Input: text (string)\n",
    "    Output: embedding vector (np.array)\n",
    "    \"\"\"\n",
    "    model = embedding\n",
    "    tokenized = simple_preprocess(text)\n",
    "    \n",
    "    word_embeddings = [np.zeros(300)]\n",
    "    for word in tokenized:\n",
    "        # if the word is in the model then embed\n",
    "        if word in model:\n",
    "            vector = model[word]\n",
    "        # add zeros for out-of-vocab words\n",
    "        else:\n",
    "            vector = np.zeros(300)\n",
    "            \n",
    "        word_embeddings.append(vector)\n",
    "    \n",
    "    # average the word vectors\n",
    "    sentence_embedding = np.stack(word_embeddings).mean(axis=0)\n",
    "    \n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288ca8eb",
   "metadata": {},
   "source": [
    "Get the vector representation of each review in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8b3469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed the sentences\n",
    "X_train_w2v = X_train.apply(lambda x: sentence2vec(text=x, embedding=w2v))\n",
    "X_test_w2v = X_test.apply(lambda x: sentence2vec(text=x, embedding=w2v))\n",
    "\n",
    "X_train_w2v = np.array(X_train_w2v.tolist())\n",
    "X_test_w2v = np.array(X_test_w2v.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fcc7fe",
   "metadata": {},
   "source": [
    "**Compile, train, and evaluate the performance of a neural network that is using word2vec word embeddings as inputs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e37fe79",
   "metadata": {},
   "source": [
    "*Compiling the neural network*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8fb07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new sequential model\n",
    "nn_w2v = tf.keras.Sequential()\n",
    "\n",
    "# Declare the hidden layers\n",
    "nn_w2v.add(layers.Dense(128, activation=\"relu\"))\n",
    "nn_w2v.add(layers.Dropout(0.2))\n",
    "nn_w2v.add(layers.Dense(128, activation=\"relu\"))\n",
    "nn_w2v.add(layers.Dropout(0.2))\n",
    "nn_w2v.add(layers.Dense(128, activation=\"relu\"))\n",
    "nn_w2v.add(layers.Dropout(0.2))\n",
    "nn_w2v.add(layers.Dense(128, activation=\"relu\"))\n",
    "nn_w2v.add(layers.Dropout(0.2))\n",
    "nn_w2v.add(layers.Dense(128, activation=\"relu\"))\n",
    "nn_w2v.add(layers.Dropout(0.2))\n",
    "\n",
    "# Declare the output layer\n",
    "nn_w2v.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "# Compile\n",
    "nn_w2v.compile(\n",
    "    # Optimizer\n",
    "    optimizer=keras.optimizers.Adam(),  \n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.MeanAbsoluteError()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd98c07",
   "metadata": {},
   "source": [
    "*Train the neural network*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a476737a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop training early if validation loss doesn't go down \n",
    "early_stop = EarlyStopping(monitor='val_loss', \n",
    "                           patience=10, \n",
    "                           mode='min', \n",
    "                           verbose=1,\n",
    "                           restore_best_weights=True)\n",
    "\n",
    "# Train\n",
    "history = nn_w2v.fit(X_train_w2v, \n",
    "                    y_train, \n",
    "                    epochs=40,\n",
    "                    verbose=1, \n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[early_stop])\n",
    "\n",
    "# Save model\n",
    "print('\\nSaving model...')\n",
    "tf.keras.models.save_model(nn_w2v, 'fitted_models/nn_w2v.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abeb0d74",
   "metadata": {},
   "source": [
    "*Evaluate the neural network*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756dcbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the network\n",
    "train_loss = history.history[\"loss\"][-1]\n",
    "result = nn_w2v.evaluate(X_test_w2v, y_test, verbose=0)\n",
    "\n",
    "print(f\"Train MAE: {train_loss:.4f}\")\n",
    "print(f\"Test MAE: {result:.4f}\") \n",
    "\n",
    "# Generate predictions\n",
    "predictions = nn_w2v.predict(X_test_w2v)\n",
    "\n",
    "# Score our predictions based on our custom clipped MAE\n",
    "print(f'The clipped MAE on the test set is {clipped_mae(y_test, predictions)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dc7cf0",
   "metadata": {},
   "source": [
    "This neural network is performing worse than both the dummy regressor, and the current best model, Ridge regression (MAE = 2.58). We will exclude this model from further investigation. \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4a2ffc",
   "metadata": {},
   "source": [
    "## 5.3. Neural network 3: GloVe\n",
    "The second neural network will use GloVe word embeddings as inputs. We will load in the word embeddings, and thenwe will convert each review to their vectorized representation, and compile, train and evaluate the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c1fd8f",
   "metadata": {},
   "source": [
    "First, we will load the GloVe word embeddings. In order to reuse our function we created for word2vec, we have to convert the GloVe vectors into the word2vec format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2af561",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "# Load GloVe embeddings in the word2vec format\n",
    "glove_file = datapath('glove.6B.300d.txt')\n",
    "word2vec_glove_file = get_tmpfile(\"glove.6B.300d.word2vec.txt\")\n",
    "glove2word2vec(glove_file, word2vec_glove_file)\n",
    "glove = KeyedVectors.load_word2vec_format(word2vec_glove_file, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c05cd60",
   "metadata": {},
   "source": [
    "Get the vector representation of each review in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d7d489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed the sentences\n",
    "X_train_glove = X_train.apply(lambda x: sentence2vec(text=x, embedding=glove))\n",
    "X_test_glove = X_test.apply(lambda x: sentence2vec(text=x, embedding=glove))\n",
    "\n",
    "X_train_glove = np.array(X_train_glove.tolist())\n",
    "X_test_glove = np.array(X_test_glove.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65379d4c",
   "metadata": {},
   "source": [
    "**Compile, train, and evaluate the performance of a neural network that is using GloVe word embeddings as inputs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5637094e",
   "metadata": {},
   "source": [
    "*Compile the network*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cfb54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new sequential model\n",
    "nn_glove = tf.keras.Sequential()\n",
    "\n",
    "# Declare the hidden layers\n",
    "nn_glove.add(layers.Dense(128, activation=\"relu\"))\n",
    "nn_glove.add(layers.Dropout(0.2))\n",
    "nn_glove.add(layers.Dense(128, activation=\"relu\"))\n",
    "nn_glove.add(layers.Dropout(0.2))\n",
    "nn_glove.add(layers.Dense(128, activation=\"relu\"))\n",
    "nn_glove.add(layers.Dropout(0.2))\n",
    "nn_glove.add(layers.Dense(128, activation=\"relu\"))\n",
    "nn_glove.add(layers.Dropout(0.2))\n",
    "nn_glove.add(layers.Dense(128, activation=\"relu\"))\n",
    "nn_glove.add(layers.Dropout(0.2))\n",
    "\n",
    "# Declare the output layer\n",
    "nn_glove.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "# Compile\n",
    "nn_glove.compile(\n",
    "    # Optimizer\n",
    "    optimizer=keras.optimizers.Adam(),  \n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.MeanAbsoluteError()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe207ff",
   "metadata": {},
   "source": [
    "*Train the network*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbe6805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop training early if validation loss doesn't go down \n",
    "early_stop = EarlyStopping(monitor='val_loss', \n",
    "                           patience=5, \n",
    "                           mode='min', \n",
    "                           verbose=1,\n",
    "                           restore_best_weights=True)\n",
    "\n",
    "# Train\n",
    "history = nn_glove.fit(X_train_glove, \n",
    "                    y_train, \n",
    "                    epochs=40,\n",
    "                    verbose=1, \n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[early_stop])\n",
    "\n",
    "# Save model\n",
    "print('\\nSaving model...')\n",
    "tf.keras.models.save_model(nn_glove, 'fitted_models/nn_glove.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be744a8b",
   "metadata": {},
   "source": [
    "*Evaluate the network*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883e2f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the network\n",
    "train_loss = history.history[\"loss\"][-1]\n",
    "result = nn_glove.evaluate(X_test_glove, y_test, verbose=0)\n",
    "\n",
    "print(f\"Train MAE: {train_loss:.4f}\")\n",
    "print(f\"Test MAE: {result:.4f}\") \n",
    "\n",
    "# Generate predictions\n",
    "predictions = nn_glove.predict(X_test_glove)\n",
    "\n",
    "# Score our predictions based on our custom clipped MAE\n",
    "print(f'The clipped MAE on the test set is {clipped_mae(y_test, predictions)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85519bc0",
   "metadata": {},
   "source": [
    "This neural network is performing worse than both the dummy regressor, and the current best model, Ridge regression (MAE = 2.58). We will exclude this model from further investigation. \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3864e79",
   "metadata": {},
   "source": [
    "## 6. Best model with word embeddings\n",
    "Since we have the word embeddings loaded, let's try fitting Ridge regression with these embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d283d880",
   "metadata": {},
   "source": [
    "### 6.1. Ridge regression with word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4854f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge with w2v\n",
    "reg_term_list = list(range(1,31))\n",
    "\n",
    "cv_scores = []\n",
    "best_score = np.inf\n",
    "best_alpha = []\n",
    "\n",
    "for alpha in reg_term_list: \n",
    "    rr = Ridge(alpha=alpha)\n",
    "    cv_score = np.mean(cross_val_score(rr, X_train_w2v, y_train, cv=5, scoring=scoring))*-1\n",
    "\n",
    "    cv_scores.append(cv_score)\n",
    "\n",
    "    if cv_score < best_score:\n",
    "        best_score = cv_score\n",
    "        best_alpha = alpha\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(reg_term_list, cv_scores, color='red', label = 'Cross Validation Score')\n",
    "plt.title('MAE for different values of alpha')\n",
    "plt.xlabel('Alpha value')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f'The lowest MAE is {best_score} with a regularization term of {best_alpha}') \n",
    "\n",
    "# recreate best model and test against test set\n",
    "rr_best = Ridge(alpha=best_alpha)\n",
    "rr_best.fit(X_train_w2v, y_train) \n",
    "\n",
    "y_pred = rr_best.predict(X_test_w2v)\n",
    "best_rr_clipped_mae = clipped_mae(y_test, y_pred)\n",
    "\n",
    "print(f'The MAE of the test set is {best_rr_clipped_mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff211f2",
   "metadata": {},
   "source": [
    "### Ridge regression with GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a236e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge with GloVe\n",
    "reg_term_list = list(range(1,31))\n",
    "\n",
    "cv_scores = []\n",
    "best_score = np.inf\n",
    "best_alpha = []\n",
    "\n",
    "for alpha in reg_term_list: \n",
    "    rr = Ridge(alpha=alpha)\n",
    "    cv_score = np.mean(cross_val_score(rr, X_train_glove, y_train, cv=5, scoring=scoring))*-1\n",
    "\n",
    "    cv_scores.append(cv_score)\n",
    "\n",
    "    if cv_score < best_score:\n",
    "        best_score = cv_score\n",
    "        best_alpha = alpha\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(reg_term_list, cv_scores, color='red', label = 'Cross Validation Score')\n",
    "plt.title('MAE for different values of alpha')\n",
    "plt.xlabel('Alpha value')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f'The lowest MAE is {best_score} with a regularization term of {best_alpha}') \n",
    "\n",
    "# recreate best model and test against test set\n",
    "rr_best = Ridge(alpha=best_alpha)\n",
    "rr_best.fit(X_train_glove, y_train) \n",
    "\n",
    "y_pred = rr_best.predict(X_test_glove)\n",
    "best_rr_clipped_mae = clipped_mae(y_test, y_pred)\n",
    "\n",
    "print(f'The MAE of the test set is {best_rr_clipped_mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ef6955",
   "metadata": {},
   "source": [
    "The last two models both perform worse than our best model, Ridge regression (MAE = 2.58). We will exclude these neural networks from further investigation. \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ce3acc",
   "metadata": {},
   "source": [
    "# Conclusion \n",
    "In this notebook we used GridSearchCV to optimize the hyperparameters of different models (Ridge regression, Lasso regression, Decision Tree Regressor, KNN Regressor, and XGB Regressor) which helped determine which of these models performed the best. GridSearchCV uses cross validation scores to determine which model performs the best. We also compared how Bag of Words and TF-IDF to see which form of text representation resulted in the best cross validation score. The best models found were Ridge regression with TF-IDF vectorization, and XGBRegressor with TF-IDF vectorization. \n",
    "\n",
    "While the XGBRegressor model performed the best (MAE=2.48), it lacks the level of interpretability that we are looking for. We want to see how individual words affect the predicted rating of a whisky, and this is not possible with XGBoost regression models. We could perhaps look at the feature importances of our XGB regression model, but that does not indicate whether a word increases or decreases a whisky's rating. Due to to lack of interpretability, this is not a model we will be investigating further.  \n",
    "\n",
    "We also fit neural networks using TF-IDF vectorization, and word2vec and GlovE word embeddings. These models did not perform as well as Ridge regression and the XGB Regressor and will not be investigated further. Please note that there was no attempt at optimizing the neural network. The main goal of this investigation was to observe the regression coefficients of whisky reviews to determine which words increase and decrease the whisky's rating. This level of interpretability is largely lost when using neural networks. We fit these models mainly out of curiousity to see if they were more accurate than our other models without tuning hyperparameters. \n",
    "\n",
    "Finally, we fit Ridge regression models using word2vec and GloVe word embeddings as well. These models also did not perform as well as Ridge regression and XGB Regressor with TF-IDF vectorization.  \n",
    "\n",
    "In our next notebook, we will further investigate Ridge regression with TF-IDF text vectorization. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
