{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cc5e77d",
   "metadata": {},
   "source": [
    "# 3. Modeling\n",
    "**Andrew Dang**  \n",
    "\n",
    "**BrainStation, Data Science**  \n",
    "\n",
    "**Previous Notebook: 2. EDA and Feature Engineering**\n",
    "\n",
    "**Next Notebook: 4. Findings**\n",
    "\n",
    "In the previous notebook, we did some initial analysis to explore the relationship between our input and target variables. \n",
    "In this notebook, we will be modeling. \n",
    "\n",
    "We are trying to predict a rating between 0 and 100, so we want to use a regression model. The evaluation metric used to determine which model performs best is the mean absolute error, which is the absolute difference the predicted rating and the actual rating averaged across the entire dataset. \n",
    "\n",
    "In this notebook, I want to test the following models. \n",
    "\n",
    "1. Decision Tree Regressor\n",
    "2. KNN Regressor\n",
    "3. Ridge Regression\n",
    "4. Lasso Regression\n",
    "5. XGBoost Regression\n",
    "5. Fully connected neural networks. \n",
    "\n",
    "As we are dealing with text data, we need to represent the text data in numeric form. There are different options for this. We will explore the following options. \n",
    "\n",
    "1. Bag of Words (CountVectorizer)\n",
    "2. TF-IDF \n",
    "3. word2Vec embeddings\n",
    "4. GloVe word embeddings\n",
    "\n",
    "Another thing to consider is whether to use stemming or lemmatization. Stemming often leads to tokens with odd or incorrect spellings that are more difficult to interpret. Therefore, we want to focus our efforts on using lemmatization. \n",
    "\n",
    "We are interested in seeing what words increase/decrease the rating of a whisky. Some words are positive or negative by definition, and don't reveal much information about the whisky. Therefore, to try and get a better understanding of whisky specific language, we want to include these positive and negative words in our list of stop words (words that are ignored and do not get tokenized). We also want to do the same thing with the name of the distillers, as their appearance in a review will not tell us much about the whisky. We will also add the names of the individual distilleries into the list of stop words. This was previosuly done in another notebook, and we will simply load them here. \n",
    "\n",
    "Now let's load in our packages. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "131ae55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data science packages\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Text data packages\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords \n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Other required packages\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "\n",
    "# Data preprocessing packages\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# modeling and metrics\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Additional NLP packages\n",
    "from sklearn.feature_extraction import text\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import plot_importance\n",
    "\n",
    "# Neural network packages\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2d750a",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bf89ea",
   "metadata": {},
   "source": [
    "## Defining functions and other utilities\n",
    "In the next few code blocks, we are doing the following:\n",
    "\n",
    "- Loading in our custom stop words that consists of positive and negative adjectives and whisky distiller names in addition to the default list of stop words.\n",
    "- Creating our lemmatizer for our text vectorizers. \n",
    "- Define a custom error and scoring function. Our scores are between 0 and 100, but there is nothing stopping our models from predicting ratings beyond these lower and upper boundaries. Our custom error function will clip all predictions below 0 to equal 0, and all predictions above 100 to 100. The custom scoring function will be passed to our models so that the predictions can be clipped during the cross-validation process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14ecc36",
   "metadata": {},
   "source": [
    "**Loading custom stop words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2354ff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open pickled custom stopwords\n",
    "my_stop_words = joblib.load('data/my_stop_words.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db16d569",
   "metadata": {},
   "source": [
    "**Creating lemmatizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fe51aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Creating lemmatizer\n",
    "# Download and instantiate the lemmatizer\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def my_lemmatizer(sentence):\n",
    "    '''\n",
    "    Takes in a string and removes punctuation, lower cases the text, and lemmatizes the text.\n",
    "    Returns a list of lemmatized words. \n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    sentence: a string\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    listoflemmad_words: list of lemmatized words. \n",
    "    '''\n",
    "    # remove punctuation and set to lower case\n",
    "    for punctuation_mark in string.punctuation:\n",
    "        sentence = sentence.replace(punctuation_mark,'').lower()\n",
    "\n",
    "    # split sentence into words\n",
    "    listofwords = sentence.split(' ')\n",
    "    listoflemmad_words = []\n",
    "    \n",
    "    # remove stopwords and any tokens that are just empty strings\n",
    "    for word in listofwords:\n",
    "        if (not word in my_stop_words and (word!='')):\n",
    "            # Stem words\n",
    "            lemmad_word = lemmatizer.lemmatize(word, pos='v')\n",
    "            listoflemmad_words.append(lemmad_word)\n",
    "\n",
    "    return listoflemmad_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38dfcaa",
   "metadata": {},
   "source": [
    "**Creating custom error and scorer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b8dbec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a custom scorer\n",
    "def clipped_mae(y_true, y_pred):\n",
    "    '''\n",
    "    A function that clips predictions above 100 to equal 100, and clip predictions under 0 to equal 0. \n",
    "    \n",
    "    Inputs:\n",
    "    -------\n",
    "    y_true: Actual label\n",
    "    y_pred: Predicted label\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    clipped_mae: The mean absolute error where predictions are clipped to remain within the boundaries of 0-100.\n",
    "    \n",
    "    '''\n",
    "    clip1 = np.where(y_pred < 0, 0, y_pred)\n",
    "    clip2 = np.where(clip1 > 100, 100, clip1)\n",
    "    clipped_mae = mean_absolute_error(y_true, clip2)\n",
    "    return clipped_mae\n",
    "\n",
    "scoring = make_scorer(clipped_mae, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684c7571",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3843643",
   "metadata": {},
   "source": [
    "## Loading fitted models\n",
    "The following function will load models that have already been fitted on the training data. This will prevent the need to refit the models every time this notebook is opened. \n",
    "\n",
    "**If you wish to see the entire fitting process, uncomment the third code block and run it.**  \n",
    "**Having the `models_loaded_flag=False` will cause each model to be re-fitted. This will take ~20-30 minutes to run.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01a7a46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fitted_models():\n",
    "    '''\n",
    "    Function that loads fitted models and sets the model_loaded_flag to True. \n",
    "    Saves readers the trouble of having to fit all the models every time they open the notebook.\n",
    "    \n",
    "    Inputs:\n",
    "    ------\n",
    "    None\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    model_dict: a dictionary that contain fitted models\n",
    "    models_loaded_flag: A boolean. If set to True, most models in the notebook will not undergo \n",
    "                        fitting, and load models from the dictionary instead.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    model_dict = {}\n",
    "    model_dict['rr_best'] = joblib.load('fitted_models/rr_best.pkl')\n",
    "    model_dict['gridsearch_model_xgb'] = joblib.load('fitted_models/gridsearch_model_xgb.pkl')\n",
    "    model_dict['gridsearch_model'] = joblib.load('fitted_models/gridsearch_model.pkl')\n",
    "    model_dict['optimized_gridsearch_model'] = joblib.load('fitted_models/optimized_gridsearch_model.pkl')\n",
    "    \n",
    "    models_loaded_flag = True\n",
    "    return model_dict, models_loaded_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecd49b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fitted models\n",
    "model_dict, models_loaded_flag = load_fitted_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6eef239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # UNCOMMENT THIS CODE BLOCK IF YOU WANT TO SEE THE TRAINING OF EACH MODEL. OTHERWISE, FITTED MODELS WILL BE LOADED\n",
    "# models_loaded_flag = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728c368e",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463d858e",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "[1. Prepare data for ColumnTransformer and Pipelines](#Step1)     \n",
    "[2. Fit a dummy regressor as a baseline model](#Step2)  \n",
    "[3. Use GridSearchCV to optimize hyperparameters for Ridge, Lasso, DecisionTreeRegressor and KNNRegressor models](#Step3)    \n",
    "[4. Use GridSearchCV to find optimal hyperparameter values for XGBRegressor](#Step4)  \n",
    "[5. Fit neural networks](#Step5)  \n",
    "*5.1. Fit neural network with TF-IDF vectorzed text*  \n",
    "*5.2. Fit neural network with word2vec word embeddings*    \n",
    "*5.3. Fit neural network with GloVe word embeddings.*  \n",
    "[6. Fit our best models from steps 3 and 4 using word embeddings](#Step6)    \n",
    "*6.1. Fit our best model using word2vec*  \n",
    "*6.2. Fit our best model using GloVe*  \n",
    "\n",
    "Models that perform worse than our dummy regressor will be excluded from further analysis. \n",
    "\n",
    "Now we will load in our preprocessed data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b4a2aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>review.point</th>\n",
       "      <th>Blended Malt Scotch Whisky</th>\n",
       "      <th>Blended Scotch Whisky</th>\n",
       "      <th>Grain Scotch Whisky</th>\n",
       "      <th>Single Grain Whisky</th>\n",
       "      <th>Single Malt Scotch</th>\n",
       "      <th>price_string</th>\n",
       "      <th>cleaned_reviews</th>\n",
       "      <th>review_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Johnnie Walker Blue Label, 40%</td>\n",
       "      <td>97</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>magnificently powerful and intense caramels dr...</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Black Bowmore, 1964 vintage, 42 year old, 40.5%</td>\n",
       "      <td>97</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>what impresses me most is how this whisky evol...</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bowmore 46 year old (distilled 1964), 42.9%</td>\n",
       "      <td>97</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13500.0</td>\n",
       "      <td>there have been some legendary bowmores from t...</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Compass Box The General, 53.4%</td>\n",
       "      <td>96</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>325.0</td>\n",
       "      <td>with a name inspired by a 1926 buster keaton m...</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chivas Regal Ultis, 40%</td>\n",
       "      <td>96</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>captivating enticing and wonderfully charming ...</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              name  review.point  \\\n",
       "0                   Johnnie Walker Blue Label, 40%            97   \n",
       "1  Black Bowmore, 1964 vintage, 42 year old, 40.5%            97   \n",
       "2      Bowmore 46 year old (distilled 1964), 42.9%            97   \n",
       "3                   Compass Box The General, 53.4%            96   \n",
       "4                          Chivas Regal Ultis, 40%            96   \n",
       "\n",
       "   Blended Malt Scotch Whisky  Blended Scotch Whisky  Grain Scotch Whisky  \\\n",
       "0                         0.0                    1.0                  0.0   \n",
       "1                         0.0                    0.0                  0.0   \n",
       "2                         0.0                    0.0                  0.0   \n",
       "3                         1.0                    0.0                  0.0   \n",
       "4                         1.0                    0.0                  0.0   \n",
       "\n",
       "   Single Grain Whisky  Single Malt Scotch  price_string  \\\n",
       "0                  0.0                 0.0         225.0   \n",
       "1                  0.0                 1.0        4500.0   \n",
       "2                  0.0                 1.0       13500.0   \n",
       "3                  0.0                 0.0         325.0   \n",
       "4                  0.0                 0.0         160.0   \n",
       "\n",
       "                                     cleaned_reviews  review_length  \n",
       "0  magnificently powerful and intense caramels dr...             66  \n",
       "1  what impresses me most is how this whisky evol...             82  \n",
       "2  there have been some legendary bowmores from t...             84  \n",
       "3  with a name inspired by a 1926 buster keaton m...             77  \n",
       "4  captivating enticing and wonderfully charming ...             71  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in data\n",
    "data = joblib.load('data/data_with_engineered_feature.pkl')\n",
    "\n",
    "# Inspect the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e03b48",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d27cb1",
   "metadata": {},
   "source": [
    "<a id='Step1'></a>\n",
    "## 1. Preparing data for ColumnTransformer\n",
    "We will separate our features into text and non-text columns. This will allow us to tell the ColumnTransformer to only vectorize and transform the text data, and allow the non-text data to pass through without any transformations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "520de1b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name',\n",
       " 'review.point',\n",
       " 'Blended Malt Scotch Whisky',\n",
       " 'Blended Scotch Whisky',\n",
       " 'Grain Scotch Whisky',\n",
       " 'Single Grain Whisky',\n",
       " 'Single Malt Scotch',\n",
       " 'price_string',\n",
       " 'review_length']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate our text and non-text data. 'name' needs to be dropped later as ColumnTransform cannot let non-numeric data pass through.\n",
    "text_data = data['cleaned_reviews']\n",
    "non_text = data.drop(['cleaned_reviews'], axis=1)\n",
    "\n",
    "# ColumnTransformer expects a list of column names as arguments. \n",
    "non_text_cols = non_text.columns.tolist()\n",
    "non_text_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b832d7",
   "metadata": {},
   "source": [
    "Now that we have separated our data into text and non-text data, we can define how individual columns will be transformed when we use the ColumnTransformer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c400732",
   "metadata": {},
   "source": [
    "## Helper Function \n",
    "Below is a function that helps us define ColumnTransformers to interact with Pipelines and GridSearchCV later. This code was borrowed from [Allistair Cota](https://github.com/allistaircota/rate_my_restaurant/blob/main/notebooks/NB3-Modelling.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4ea6bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_col_trans(input_text, vectorizer):\n",
    "    '''\n",
    "    Returns a ColumnTransformer which first performs a \n",
    "    passthrough on the numeric columns, then applies\n",
    "    a vectorizer on the `text` column\n",
    "    \n",
    "    PARAMETERS:\n",
    "    - input_text: str, to name the vectorizer tuple\n",
    "    - vectorizer: Sklearn text vectorizer\n",
    "    \n",
    "    RETURNS:\n",
    "    - col_trans: sklearn ColumnTransformer\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Vectorize text data (cleaned_reviews column) and let everything else pass through\n",
    "    \n",
    "    col_trans = ColumnTransformer([\n",
    "        (input_text, vectorizer, 'cleaned_reviews')\n",
    "    ], \n",
    "        remainder='passthrough', \n",
    "        sparse_threshold=0)\n",
    "    \n",
    "    return col_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f14be01",
   "metadata": {},
   "source": [
    "## Splitting the data\n",
    "\n",
    "### Note about dropping `name`\n",
    "Name requires to be dropped from the input variables as it isn't actually used for analysis, and causes issues when we scale the data (as it is not converted into numeric form). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a85c8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign and inputs and target to variables \n",
    "X = data.drop(['name', 'review.point'], axis=1)\n",
    "y = data['review.point']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=88)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68cea77",
   "metadata": {},
   "source": [
    "## Preparing the pipeline\n",
    "We will use the pipeline to vectorize our text data, and then scale our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91295f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column transformers\n",
    "bow_ct = define_col_trans('bow_ct',  CountVectorizer(stop_words=my_stop_words, min_df=5, tokenizer=my_lemmatizer))\n",
    "tfidf_ct = define_col_trans('tfidf_ct',  TfidfVectorizer(stop_words=my_stop_words, min_df=5, tokenizer=my_lemmatizer))\n",
    "\n",
    "vectorizer_list = [bow_ct, tfidf_ct]\n",
    "\n",
    "# Prepare pipeline - vectorize text, scale data, and then fit a model\n",
    "estimators = [\n",
    "    ('transformer', bow_ct), \n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', Ridge())\n",
    "]\n",
    "\n",
    "my_pipe = Pipeline(estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29ef8d9",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691891c9",
   "metadata": {},
   "source": [
    "<a id='Step2'></a>\n",
    "## 2. Baseline model\n",
    "We will use a model that always predicts the mean rating as a baseline model. Any model that performs worse than the baseline will not undergo further analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "532cb74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The baseline model that always predicts the mean has an MAE of: 3.1338969052732875\n"
     ]
    }
   ],
   "source": [
    "# Dummy regressor - Baseline model \n",
    "baseline = DummyRegressor(strategy='mean')\n",
    "baseline.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_pred = baseline.predict(X_test)\n",
    "\n",
    "# use MAE to determine baseline score \n",
    "dummy_mae = clipped_mae(y_test, y_pred) \n",
    "\n",
    "print(f'The baseline model that always predicts the mean has an MAE of: {dummy_mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6802bb1d",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bacd4e",
   "metadata": {},
   "source": [
    "<a id='Step3'></a>\n",
    "## 3. Using GridSearchCV: Finding the model with the lowest error\n",
    "In the next code block, we will be using GridSearchCV to see which model between Ridge regression, Lasso regression, Decision Tree Regressor, and KNN regressor has the lowest cross validation score. We will also try different text representations - Tfidf and CountVectorizer - and see which representation results in the lowest cross validation score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "debb720f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained model...\n",
      "\n",
      "The best model used the following settings: \n",
      " Pipeline(steps=[('transformer',\n",
      "                 ColumnTransformer(remainder='passthrough', sparse_threshold=0,\n",
      "                                   transformers=[('tfidf_ct',\n",
      "                                                  TfidfVectorizer(min_df=5,\n",
      "                                                                  stop_words=frozenset({'a',\n",
      "                                                                                        'aberfeldy',\n",
      "                                                                                        'aberlour',\n",
      "                                                                                        'about',\n",
      "                                                                                        'above',\n",
      "                                                                                        'across',\n",
      "                                                                                        'adelphi',\n",
      "                                                                                        'after',\n",
      "                                                                                        'afterwards',\n",
      "                                                                                        'again',\n",
      "                                                                                        'against',\n",
      "                                                                                        'alexander',\n",
      "                                                                                        'all',\n",
      "                                                                                        'alltabhainne',\n",
      "                                                                                        'almost',\n",
      "                                                                                        'alone',\n",
      "                                                                                        'along',\n",
      "                                                                                        'already',\n",
      "                                                                                        'also',\n",
      "                                                                                        'although',\n",
      "                                                                                        'always',\n",
      "                                                                                        'am',\n",
      "                                                                                        'among',\n",
      "                                                                                        'amongst',\n",
      "                                                                                        'amoungst',\n",
      "                                                                                        'amount',\n",
      "                                                                                        'an',\n",
      "                                                                                        'and',\n",
      "                                                                                        'another',\n",
      "                                                                                        'antiquary', ...}),\n",
      "                                                                  tokenizer=<function my_lemmatizer at 0x000001CED1850DC8>),\n",
      "                                                  'cleaned_reviews')])),\n",
      "                ('scaler', MinMaxScaler()), ('model', Ridge(alpha=10))])\n",
      "The test set accuracy of the best model from our grid search is 2.599054592308857\n"
     ]
    }
   ],
   "source": [
    "# If models have been loaded, grab it from dictionary. Otherwise, perform gridsearch. \n",
    "if models_loaded_flag:\n",
    "    print('Loading pre-trained model...')\n",
    "    gridsearch_model = model_dict['gridsearch_model']\n",
    "    \n",
    "    # Print the best estimator\n",
    "    print('\\nThe best model used the following settings:', '\\n',gridsearch_model.best_estimator_)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_preds = gridsearch_model.predict(X_test)\n",
    "\n",
    "    # Print score \n",
    "    print(f'The test set accuracy of the best model from our grid search is {clipped_mae(y_preds, y_test)}')\n",
    "\n",
    "else: \n",
    "    print('Training models from scratch')\n",
    "    # List of hyperparameters to tune\n",
    "    c_values = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "    K_values = np.arange(1,32,2).tolist()\n",
    "    depth_list = [2,3,4,5,6,7]\n",
    "\n",
    "    # Define parameters of GridSearch \n",
    "    # Try 4 different models with different hyperparameter options, as well as 2 different vectorizers and scalers. \n",
    "    param_grid = [\n",
    "                {'transformer': vectorizer_list,\n",
    "                 'model': [Ridge()],\n",
    "                 'scaler': [StandardScaler(), MinMaxScaler()],\n",
    "                 'model__alpha': c_values},\n",
    "                {'transformer': vectorizer_list,\n",
    "                 'model': [Lasso()],\n",
    "                 'scaler': [StandardScaler(), MinMaxScaler()],\n",
    "                 'model__alpha': c_values},\n",
    "                {'transformer': vectorizer_list,\n",
    "                 'model': [KNeighborsRegressor()],\n",
    "                 'scaler': [StandardScaler(), MinMaxScaler()],\n",
    "                 'model__n_neighbors': K_values },\n",
    "                {'transformer': vectorizer_list,\n",
    "                 'model': [DecisionTreeRegressor()],\n",
    "                 'scaler': [StandardScaler(), MinMaxScaler()],\n",
    "                 'model__max_depth': depth_list}\n",
    "    ]\n",
    "    \n",
    "    # GridSearch for best estimator\n",
    "    my_grid = GridSearchCV(my_pipe, param_grid, cv=5, scoring=scoring, n_jobs=-1)\n",
    "    my_fitted_grid = my_grid.fit(X_train, y_train)\n",
    "\n",
    "    # Best estimator\n",
    "    print(my_fitted_grid.best_estimator_)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_preds = my_fitted_grid.predict(X_test)\n",
    "\n",
    "    # Print score \n",
    "    print(f'\\n The test set accuracy of the best model from our grid search is {clipped_mae(y_preds, y_test)}')\n",
    "\n",
    "    # Save best estimator\n",
    "    grid_search_model = my_fitted_grid\n",
    "    joblib.dump(my_fitted_grid, 'fitted_models/gridsearch_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2babc2",
   "metadata": {},
   "source": [
    "<a id='Step31'></a>\n",
    "\n",
    "### 3.1 Optimizing best model\n",
    "\n",
    "The output of the grid search revealed that the best model was a Ridge regression model using a TfidfVectorizer, a MinMaxScaler, and a regularization term of 10. In the next block, we will try to further optimize the regularization term. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d116bb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained model...\n",
      "\n",
      "The optimal model used the following parameters: \n",
      " Pipeline(steps=[('transformer',\n",
      "                 ColumnTransformer(remainder='passthrough', sparse_threshold=0,\n",
      "                                   transformers=[('bow_ct',\n",
      "                                                  CountVectorizer(min_df=5,\n",
      "                                                                  stop_words=frozenset({'a',\n",
      "                                                                                        'aberfeldy',\n",
      "                                                                                        'aberlour',\n",
      "                                                                                        'about',\n",
      "                                                                                        'above',\n",
      "                                                                                        'across',\n",
      "                                                                                        'adelphi',\n",
      "                                                                                        'after',\n",
      "                                                                                        'afterwards',\n",
      "                                                                                        'again',\n",
      "                                                                                        'against',\n",
      "                                                                                        'alexander',\n",
      "                                                                                        'all',\n",
      "                                                                                        'alltabhainne',\n",
      "                                                                                        'almost',\n",
      "                                                                                        'alone',\n",
      "                                                                                        'along',\n",
      "                                                                                        'already',\n",
      "                                                                                        'also',\n",
      "                                                                                        'although',\n",
      "                                                                                        'always',\n",
      "                                                                                        'am',\n",
      "                                                                                        'among',\n",
      "                                                                                        'amongst',\n",
      "                                                                                        'amoungst',\n",
      "                                                                                        'amount',\n",
      "                                                                                        'an',\n",
      "                                                                                        'and',\n",
      "                                                                                        'another',\n",
      "                                                                                        'antiquary', ...}),\n",
      "                                                                  tokenizer=<function my_lemmatizer at 0x000001CED1850DC8>),\n",
      "                                                  'cleaned_reviews')])),\n",
      "                ('scaler', MinMaxScaler()), ('model', Ridge(alpha=20))])\n",
      "\n",
      "The clipped MAE on the test set is 2.616718588409604\n"
     ]
    }
   ],
   "source": [
    "if models_loaded_flag:\n",
    "    print('Loading pre-trained model...')\n",
    "    optimized_gridsearch_model = model_dict['optimized_gridsearch_model']\n",
    "    \n",
    "    # Print optimized model \n",
    "    print('\\nThe optimal model used the following parameters:', '\\n', optimized_gridsearch_model.best_estimator_)\n",
    "    \n",
    "    # Predict on test set\n",
    "    y_preds = optimized_gridsearch_model.predict(X_test)\n",
    "    \n",
    "    # Clipped MAE on test set\n",
    "    print(f'\\nThe clipped MAE on the test set is {clipped_mae(y_preds, y_test)}')\n",
    "\n",
    "else:     \n",
    "    \n",
    "    # Additional values for regularization term to try\n",
    "    c_list = np.arange(10,41).tolist()\n",
    "\n",
    "    # Try to optimize Regularization  \n",
    "    my_pipe2 = Pipeline(estimators)\n",
    "\n",
    "    param_grid2 = [\n",
    "                {'transformer': [tfidf_ct],\n",
    "                 'model': [Ridge()],\n",
    "                 'scaler': [MinMaxScaler()],\n",
    "                 'model__alpha': c_list}\n",
    "    ]\n",
    "    \n",
    "    # GridSearch for best estimator\n",
    "    grid2 = GridSearchCV(my_pipe2, param_grid2, cv=5, scoring=scoring, n_jobs=-1)\n",
    "    fitted_grid2 = grid2.fit(X_train, y_train)\n",
    "\n",
    "    # Score of best model \n",
    "    y_preds = fitted_grid2.predict(X_test)\n",
    "\n",
    "    # Clipped mae \n",
    "    print(f'\\nThe test MAE of the best model is {clipped_mae(y_test, y_preds)}')\n",
    "\n",
    "    # Best estimator\n",
    "    print(fitted_grid2.best_estimator_)\n",
    "\n",
    "    # Save optimized model \n",
    "    optimized_gridsearch_model = fitted_grid2\n",
    "    joblib.dump(optimized_gridsearch_model, 'optimized_gridsearch_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e017c4",
   "metadata": {},
   "source": [
    "From the output above, after further tuning our regularization term, the best performing Ridge regression model had a regularization term of 20. \n",
    "We will recreate the best ridge regression and model and save it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62ee160",
   "metadata": {},
   "source": [
    "## 3.2. Recreating and saving best model\n",
    "Below we will be recreating the best Ridge regression model and saving it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e06564c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained model...\n",
      "\n",
      "The MAE of the test set is 2.6120779945039247\n"
     ]
    }
   ],
   "source": [
    "# If models are loaded, extract the fitted model from the models dictionary. If they are not loaded, train the model. \n",
    "if models_loaded_flag:\n",
    "    print('Loading pre-trained model...')\n",
    "    \n",
    "    # load model from dictionary\n",
    "    rr_best = model_dict['rr_best']\n",
    "    \n",
    "    # make predictions on our fitted model\n",
    "    y_pred = rr_best.predict(X_test)\n",
    "    \n",
    "    # calculate MAE on test set\n",
    "    best_rr_clipped_mae = clipped_mae(y_test, y_pred)\n",
    "    \n",
    "    print(f'\\nThe MAE of the test set is {best_rr_clipped_mae}')\n",
    "\n",
    "else: \n",
    "\n",
    "    ridge_pipe = Pipeline([\n",
    "        ('transformer', tfidf_ct),\n",
    "        ('scaler', MinMaxScaler()),\n",
    "        ('model', Ridge(alpha=20))\n",
    "    ])\n",
    "    \n",
    "    # Fit best model\n",
    "    rr_best = ridge_pipe.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions with model \n",
    "    y_preds = rr_best.predict(X_test)\n",
    "    \n",
    "    # MAE on test set\n",
    "    print(f'\\nThe MAE of the test set is {clipped_mae(y_test, y_preds)}')\n",
    "\n",
    "    # Pickle our best model \n",
    "    joblib.dump(rr_best, 'fitted_models/rr_best.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d85c93b",
   "metadata": {},
   "source": [
    "### Best model so far: Ridge regression with TF-IDF\n",
    "So far we have used GridSearch to rule out 3 different models. We found that Ridge regression had the best cross validation score. When we compared TF-IDF tokenization against Bag of Words tokenization, Bag of Words had a lower mean absolute error on the test set (3.05). We tried to optimize the Ridge regression model by testing more values for the regularization term, and found that the best model used TF-IDF representation (MAE = 2.612). The Dummy regressor had a mean absoute error of 3.133, so the Ridge regression model is our candidate for best model so far. \n",
    "\n",
    "The next step is to investigate ensemble methods and neural networks. \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457134d6",
   "metadata": {},
   "source": [
    "<a id='Step4'></a>\n",
    "## 4. XGBRegressor \n",
    "We are interested in the difference in language used between high scoring whiskys, and lower scoring whiskys. The manner in which we do this will be to investigate the coefficient of each word in our vocabulary. While ensemble methods and neural networks may provide a more accurate model, we lose ease of interpretability with these models. \n",
    "\n",
    "With that in mind, we will fit an XGBRegressor model and try fitting different neural networks and see how accurate our predictions can get.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fa1c48",
   "metadata": {},
   "source": [
    "In the code block below, we will use GridSearchCV to help us optimze the hyperparameters for the XGBRegressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89a3d93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained model...\n",
      "\n",
      "The best estimator used the following parameters: \n",
      " Pipeline(steps=[('transformer',\n",
      "                 ColumnTransformer(remainder='passthrough', sparse_threshold=0,\n",
      "                                   transformers=[('bow_ct',\n",
      "                                                  CountVectorizer(min_df=5,\n",
      "                                                                  stop_words=frozenset({'a',\n",
      "                                                                                        'aberfeldy',\n",
      "                                                                                        'aberlour',\n",
      "                                                                                        'about',\n",
      "                                                                                        'above',\n",
      "                                                                                        'across',\n",
      "                                                                                        'adelphi',\n",
      "                                                                                        'after',\n",
      "                                                                                        'afterwards',\n",
      "                                                                                        'again',\n",
      "                                                                                        'against',\n",
      "                                                                                        'alexander',\n",
      "                                                                                        'all',\n",
      "                                                                                        'alltabhainne',\n",
      "                                                                                        'almost',\n",
      "                                                                                        'alone',\n",
      "                                                                                        'along',\n",
      "                                                                                        'already',\n",
      "                                                                                        'also',\n",
      "                                                                                        'although',...\n",
      "                              gamma=0, gpu_id=-1, importance_type=None,\n",
      "                              interaction_constraints='', learning_rate=0.3,\n",
      "                              max_delta_step=0, max_depth=1, min_child_weight=1,\n",
      "                              missing=nan, monotone_constraints='()',\n",
      "                              n_estimators=200, n_jobs=8, num_parallel_tree=1,\n",
      "                              predictor='auto', random_state=0, reg_alpha=0,\n",
      "                              reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "                              tree_method='exact', validate_parameters=1,\n",
      "                              verbosity=None))])\n",
      "\n",
      "The MAE on the test set is 2.5041963029279475\n"
     ]
    }
   ],
   "source": [
    "# If models are loaded, extract the fitted model from the models dictionary. If they are not loaded, optimize the hyperparameters. \n",
    "if models_loaded_flag:\n",
    "    print('Loading pre-trained model...')\n",
    "    gridsearch_model_xgb = model_dict['gridsearch_model_xgb']\n",
    "    \n",
    "    # Print the best estimator\n",
    "    print('\\nThe best estimator used the following parameters:', '\\n', gridsearch_model_xgb.best_estimator_)\n",
    "    \n",
    "    # Test MAE of the best model \n",
    "    predictions = gridsearch_model_xgb.predict(X_test)\n",
    "    print(f'\\nThe MAE on the test set is {clipped_mae(y_test, predictions)}')\n",
    "    \n",
    "else:    \n",
    "\n",
    "    # Using GridSearchCV to find the optimal values for XGBRegressor hyperparameters\n",
    "    # Setup the pipeline\n",
    "    estimators = [\n",
    "        ('transformer', tfidf_ct),\n",
    "        ('scaler', MinMaxScaler()),\n",
    "        ('model', XGBRegressor())\n",
    "    ]\n",
    "    \n",
    "    pipe = Pipeline(estimators)\n",
    "    \n",
    "    # Define which hyperparameters we want to tune\n",
    "    params = {\n",
    "              'transformer': vectorizer_list,\n",
    "              'scaler': [MinMaxScaler(), StandardScaler()],\n",
    "              'model__max_depth': [1, 2, 3, 4],\n",
    "              'model__learning_rate': [0.01, 0.1, 0.3],\n",
    "              'model__n_estimators': [100, 200]\n",
    "    }\n",
    "\n",
    "    # Fit GridSearchCV\n",
    "    grid_search = GridSearchCV(pipe, param_grid=params, scoring=scoring, n_jobs=-1)\n",
    "    fitted_search = grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Print the optial hyperparameter values \n",
    "    print(fitted_search.best_estimator_)\n",
    "    \n",
    "    # Test MAE of the best model \n",
    "    predictions = fitted_search.predict(X_test)\n",
    "    print(f'\\nThe MAE on the test set is {clipped_mae(y_test, predictions)}')\n",
    "\n",
    "    # Save to pickle\n",
    "    gridsearch_model_xgb = fitted_search\n",
    "    joblib.dump(gridsearch_model_xgb, 'fitted_models/gridsearch_model_xgb.pkl')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b5eb42",
   "metadata": {},
   "source": [
    "The XGBRegressor model has a better test mean absolute error than our Ridge regression model. We will fit a new XGBRegressor with the optimized hyperparameter values and save the fitted model so we can further investigate the model later on. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be24970",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8049db9",
   "metadata": {},
   "source": [
    "<a id='Step5'></a>\n",
    "## 5. Fit Neural Networks\n",
    "We will fit neural networks with the following text representations:\n",
    "1. TF-IDF\n",
    "2. word2vec word embeddings\n",
    "3. Global Vectors for Word Representation (GloVe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259be061",
   "metadata": {},
   "source": [
    "### 5.1. Neural network 1: TF-IDF\n",
    "In the next few code blocks, we are compiling, training, and evaluating a neural network that is using TF-IDF tokenized text as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d8622ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.metrics import MeanAbsoluteError\n",
    "\n",
    "# Wrap TF model as scikit-learn model to make use of Pipelines\n",
    "def create_model(optimizer='adam',\n",
    "                 kernel_initializer='glorot_uniform', \n",
    "                 dropout=0.2):\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(128,activation='relu', kernel_initializer=kernel_initializer))\n",
    "    model.add(layers.Dropout(dropout))\n",
    "    model.add(layers.Dense(128,activation='relu', kernel_initializer=kernel_initializer))\n",
    "    model.add(layers.Dropout(dropout))\n",
    "    model.add(layers.Dense(128, activation=\"relu\", kernel_initializer=kernel_initializer))\n",
    "    model.add(layers.Dropout(dropout))\n",
    "    model.add(layers.Dense(128, activation=\"relu\", kernel_initializer=kernel_initializer))\n",
    "    model.add(layers.Dropout(dropout))\n",
    "\n",
    "    # Declare the output layer\n",
    "    model.add(layers.Dense(1, activation='linear', kernel_initializer=kernel_initializer))\n",
    "    \n",
    "    model.compile(loss=keras.losses.MeanAbsoluteError(), \n",
    "                  optimizer=optimizer,\n",
    "                  metrics=[MeanAbsoluteError()])\n",
    "\n",
    "    return model\n",
    "\n",
    "# wrap the model using the function you created\n",
    "model = KerasRegressor(build_fn=create_model,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53236254",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test MAE is 5.284430200113751\n"
     ]
    }
   ],
   "source": [
    "# Set up pipeline\n",
    "nn_pipe = Pipeline([\n",
    "    ('vectorizer', tfidf_ct),\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "# Define early stopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', \n",
    "                       patience=10, \n",
    "                       mode='min', \n",
    "                       verbose=1,\n",
    "                       restore_best_weights=True)\n",
    "\n",
    "# Fit pipe\n",
    "nn_pipe.fit(\n",
    "    X_train, y_train, \n",
    "    model__epochs=40,\n",
    "    model__validation_split=0.2, \n",
    "    model__callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "# Make prediction\n",
    "nn_preds = nn_pipe.predict(X_test)\n",
    "\n",
    "# Check MAE \n",
    "print(f'The test MAE is {clipped_mae(nn_preds, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120646e5",
   "metadata": {},
   "source": [
    "*Model evaluation*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516529b3",
   "metadata": {},
   "source": [
    "This neural network is performing worse than both the dummy regressor, and the current best model, Ridge regression (MAE = 2.62). We will exclude this model from further investigation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6b51e2",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d411d9fe",
   "metadata": {},
   "source": [
    "## 5.2. Neural network 2: word2vec\n",
    "The second neural network will use word2vec word embeddings as inputs. We will load in the word embeddings, and create a function that will calculate the vectorized representation of each review. Then, we will convert each review to their vectorized representation, and compile, train and evaluate the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b25e647",
   "metadata": {},
   "source": [
    "First, we will have to load the word embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24112ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load word2vec word embeddings\n",
    "w2v = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    'lexvec.enwiki+newscrawl.300d.W.pos.vectors', binary=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571642dc",
   "metadata": {},
   "source": [
    "Just use text data when using word embeddings. Let's use the original, unprocessed data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5f31315",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = pd.read_csv('data/scotch_review.csv')\n",
    "X = original_data['description']\n",
    "y = original_data['review.point']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f225196",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=88)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12650ad8",
   "metadata": {},
   "source": [
    "Define a function that takes the average word embedding for the entire review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8685b758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2vec(text, embedding):\n",
    "    \"\"\"\n",
    "    Embed a sentence by averaging the word vectors of the tokenized text. Out-of-vocabulary words are replaced by the zero-vector.\n",
    "    -----\n",
    "    \n",
    "    Input: text (string)\n",
    "    Output: embedding vector (np.array)\n",
    "    \"\"\"\n",
    "    model = embedding\n",
    "    tokenized = simple_preprocess(text)\n",
    "    \n",
    "    word_embeddings = [np.zeros(300)]\n",
    "    for word in tokenized:\n",
    "        # if the word is in the model then embed\n",
    "        if word in model:\n",
    "            vector = model[word]\n",
    "        # add zeros for out-of-vocab words\n",
    "        else:\n",
    "            vector = np.zeros(300)\n",
    "            \n",
    "        word_embeddings.append(vector)\n",
    "    \n",
    "    # average the word vectors\n",
    "    sentence_embedding = np.stack(word_embeddings).mean(axis=0)\n",
    "    \n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288ca8eb",
   "metadata": {},
   "source": [
    "Get the vector representation of each review in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f8b3469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed the sentences\n",
    "X_train_w2v = X_train.apply(lambda x: sentence2vec(text=x, embedding=w2v))\n",
    "X_test_w2v = X_test.apply(lambda x: sentence2vec(text=x, embedding=w2v))\n",
    "\n",
    "X_train_w2v = np.array(X_train_w2v.tolist())\n",
    "X_test_w2v = np.array(X_test_w2v.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fcc7fe",
   "metadata": {},
   "source": [
    "**Compile, train, and evaluate the performance of a neural network that is using word2vec word embeddings as inputs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e37fe79",
   "metadata": {},
   "source": [
    "*Compiling the neural network*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d8fb07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new sequential model\n",
    "nn_w2v = tf.keras.Sequential()\n",
    "\n",
    "# Declare the hidden layers\n",
    "nn_w2v.add(layers.Dense(128, activation=\"relu\"))\n",
    "nn_w2v.add(layers.Dropout(0.2))\n",
    "nn_w2v.add(layers.Dense(128, activation=\"relu\"))\n",
    "nn_w2v.add(layers.Dropout(0.2))\n",
    "nn_w2v.add(layers.Dense(128, activation=\"relu\"))\n",
    "nn_w2v.add(layers.Dropout(0.2))\n",
    "nn_w2v.add(layers.Dense(128, activation=\"relu\"))\n",
    "nn_w2v.add(layers.Dropout(0.2))\n",
    "nn_w2v.add(layers.Dense(128, activation=\"relu\"))\n",
    "nn_w2v.add(layers.Dropout(0.2))\n",
    "\n",
    "# Declare the output layer\n",
    "nn_w2v.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "# Compile\n",
    "nn_w2v.compile(\n",
    "    # Optimizer\n",
    "    optimizer=keras.optimizers.Adam(),  \n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.MeanAbsoluteError()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd98c07",
   "metadata": {},
   "source": [
    "*Train the neural network*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a476737a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "45/45 [==============================] - 1s 6ms/step - loss: 47.1009 - val_loss: 4.8880\n",
      "Epoch 2/40\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 11.2456 - val_loss: 6.9670\n",
      "Epoch 3/40\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 11.1641 - val_loss: 11.5767\n",
      "Epoch 4/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 10.7048 - val_loss: 7.8047\n",
      "Epoch 5/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 10.6538 - val_loss: 7.5353\n",
      "Epoch 6/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 10.3638 - val_loss: 3.4544\n",
      "Epoch 7/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 9.3551 - val_loss: 4.5704\n",
      "Epoch 8/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 9.6375 - val_loss: 3.4737\n",
      "Epoch 9/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 9.0388 - val_loss: 3.0685\n",
      "Epoch 10/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 9.1504 - val_loss: 6.0561\n",
      "Epoch 11/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 8.8847 - val_loss: 5.7291\n",
      "Epoch 12/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 8.5325 - val_loss: 5.6418\n",
      "Epoch 13/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 8.5138 - val_loss: 5.5551\n",
      "Epoch 14/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 8.3634 - val_loss: 5.8484\n",
      "Epoch 15/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 8.4012 - val_loss: 4.2109\n",
      "Epoch 16/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 8.1286 - val_loss: 3.6024\n",
      "Epoch 17/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 7.7584 - val_loss: 9.4082\n",
      "Epoch 18/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 8.1900 - val_loss: 2.7502\n",
      "Epoch 19/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 7.4443 - val_loss: 5.6892\n",
      "Epoch 20/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 7.8502 - val_loss: 2.7868\n",
      "Epoch 21/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 7.8337 - val_loss: 2.7678\n",
      "Epoch 22/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 7.9193 - val_loss: 8.4624\n",
      "Epoch 23/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 7.7054 - val_loss: 2.9788\n",
      "Epoch 24/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 7.2731 - val_loss: 4.1138\n",
      "Epoch 25/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 7.1034 - val_loss: 4.0815\n",
      "Epoch 26/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 7.2278 - val_loss: 3.5006\n",
      "Epoch 27/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 7.1819 - val_loss: 4.2847\n",
      "Epoch 28/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 7.3565 - val_loss: 3.3009\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00028: early stopping\n",
      "\n",
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "# Stop training early if validation loss doesn't go down \n",
    "early_stop = EarlyStopping(monitor='val_loss', \n",
    "                           patience=10, \n",
    "                           mode='min', \n",
    "                           verbose=1,\n",
    "                           restore_best_weights=True)\n",
    "\n",
    "# Train\n",
    "history = nn_w2v.fit(X_train_w2v, \n",
    "                    y_train, \n",
    "                    epochs=40,\n",
    "                    verbose=1, \n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[early_stop])\n",
    "\n",
    "# Save model\n",
    "print('\\nSaving model...')\n",
    "tf.keras.models.save_model(nn_w2v, 'fitted_models/nn_w2v.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abeb0d74",
   "metadata": {},
   "source": [
    "*Evaluate the neural network*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "756dcbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MAE: 7.3565\n",
      "Test MAE: 3.0204\n",
      "\n",
      "The clipped MAE on the test set is 3.020440165201823\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the network\n",
    "train_loss = history.history[\"loss\"][-1]\n",
    "result = nn_w2v.evaluate(X_test_w2v, y_test, verbose=0)\n",
    "\n",
    "print(f\"Train MAE: {train_loss:.4f}\")\n",
    "print(f\"Test MAE: {result:.4f}\") \n",
    "\n",
    "# Generate predictions\n",
    "predictions = nn_w2v.predict(X_test_w2v)\n",
    "\n",
    "# Score our predictions based on our custom clipped MAE\n",
    "print(f'\\nThe clipped MAE on the test set is {clipped_mae(y_test, predictions)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dc7cf0",
   "metadata": {},
   "source": [
    "This neural network is performing worse than both the dummy regressor, and the current best model, Ridge regression (MAE = 2.58). We will exclude this model from further investigation. \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4a2ffc",
   "metadata": {},
   "source": [
    "## 5.3. Neural network 3: GloVe\n",
    "The second neural network will use GloVe word embeddings as inputs. We will load in the word embeddings, and thenwe will convert each review to their vectorized representation, and compile, train and evaluate the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c1fd8f",
   "metadata": {},
   "source": [
    "First, we will load the GloVe word embeddings. In order to reuse our function we created for word2vec, we have to convert the GloVe vectors into the word2vec format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c2af561",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "# Load GloVe embeddings in the word2vec format\n",
    "glove_file = datapath('glove.6B.300d.txt')\n",
    "word2vec_glove_file = get_tmpfile(\"glove.6B.300d.word2vec.txt\")\n",
    "glove2word2vec(glove_file, word2vec_glove_file)\n",
    "glove = KeyedVectors.load_word2vec_format(word2vec_glove_file, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c05cd60",
   "metadata": {},
   "source": [
    "Get the vector representation of each review in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c4d7d489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed the sentences\n",
    "X_train_glove = X_train.apply(lambda x: sentence2vec(text=x, embedding=glove))\n",
    "X_test_glove = X_test.apply(lambda x: sentence2vec(text=x, embedding=glove))\n",
    "\n",
    "X_train_glove = np.array(X_train_glove.tolist())\n",
    "X_test_glove = np.array(X_test_glove.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65379d4c",
   "metadata": {},
   "source": [
    "**Compile, train, and evaluate the performance of a neural network that is using GloVe word embeddings as inputs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5637094e",
   "metadata": {},
   "source": [
    "*Compile the network*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "79cfb54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new sequential model\n",
    "nn_glove = tf.keras.Sequential()\n",
    "\n",
    "# Declare the hidden layers\n",
    "nn_glove.add(layers.Dense(128, activation=\"relu\"))\n",
    "nn_glove.add(layers.Dropout(0.2))\n",
    "nn_glove.add(layers.Dense(128, activation=\"relu\"))\n",
    "nn_glove.add(layers.Dropout(0.2))\n",
    "nn_glove.add(layers.Dense(128, activation=\"relu\"))\n",
    "nn_glove.add(layers.Dropout(0.2))\n",
    "nn_glove.add(layers.Dense(128, activation=\"relu\"))\n",
    "nn_glove.add(layers.Dropout(0.2))\n",
    "nn_glove.add(layers.Dense(128, activation=\"relu\"))\n",
    "nn_glove.add(layers.Dropout(0.2))\n",
    "\n",
    "# Declare the output layer\n",
    "nn_glove.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "# Compile\n",
    "nn_glove.compile(\n",
    "    # Optimizer\n",
    "    optimizer=keras.optimizers.Adam(),  \n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.MeanAbsoluteError()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe207ff",
   "metadata": {},
   "source": [
    "*Train the network*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0cbe6805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "45/45 [==============================] - 1s 8ms/step - loss: 48.3929 - val_loss: 6.2093\n",
      "Epoch 2/40\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 12.0593 - val_loss: 5.2802\n",
      "Epoch 3/40\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 12.0593 - val_loss: 10.6977\n",
      "Epoch 4/40\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 11.4243 - val_loss: 5.1628\n",
      "Epoch 5/40\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 12.2873 - val_loss: 9.1291\n",
      "Epoch 6/40\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 10.9900 - val_loss: 4.9930\n",
      "Epoch 7/40\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 10.7319 - val_loss: 7.8148\n",
      "Epoch 8/40\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 10.3335 - val_loss: 4.0346\n",
      "Epoch 9/40\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 9.7696 - val_loss: 4.9076\n",
      "Epoch 10/40\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 9.5248 - val_loss: 8.0471\n",
      "Epoch 11/40\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 9.7624 - val_loss: 6.8902\n",
      "Epoch 12/40\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 8.9498 - val_loss: 5.3330\n",
      "Epoch 13/40\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 9.2577 - val_loss: 3.5727\n",
      "Epoch 14/40\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 9.3674 - val_loss: 11.2523\n",
      "Epoch 15/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 9.0248 - val_loss: 5.5085\n",
      "Epoch 16/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 8.6069 - val_loss: 8.3544\n",
      "Epoch 17/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 8.4403 - val_loss: 3.9345\n",
      "Epoch 18/40\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 8.8496 - val_loss: 9.4387\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00018: early stopping\n",
      "\n",
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "# Stop training early if validation loss doesn't go down \n",
    "early_stop = EarlyStopping(monitor='val_loss', \n",
    "                           patience=5, \n",
    "                           mode='min', \n",
    "                           verbose=1,\n",
    "                           restore_best_weights=True)\n",
    "\n",
    "# Train\n",
    "history = nn_glove.fit(X_train_glove, \n",
    "                    y_train, \n",
    "                    epochs=40,\n",
    "                    verbose=1, \n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[early_stop])\n",
    "\n",
    "# Save model\n",
    "print('\\nSaving model...')\n",
    "tf.keras.models.save_model(nn_glove, 'fitted_models/nn_glove.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be744a8b",
   "metadata": {},
   "source": [
    "*Evaluate the network*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "883e2f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MAE: 8.8496\n",
      "Test MAE: 3.7293\n",
      "\n",
      "The clipped MAE on the test set is 3.7292915513780382\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the network\n",
    "train_loss = history.history[\"loss\"][-1]\n",
    "result = nn_glove.evaluate(X_test_glove, y_test, verbose=0)\n",
    "\n",
    "print(f\"Train MAE: {train_loss:.4f}\")\n",
    "print(f\"Test MAE: {result:.4f}\") \n",
    "\n",
    "# Generate predictions\n",
    "predictions = nn_glove.predict(X_test_glove)\n",
    "\n",
    "# Score our predictions based on our custom clipped MAE\n",
    "print(f'\\nThe clipped MAE on the test set is {clipped_mae(y_test, predictions)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85519bc0",
   "metadata": {},
   "source": [
    "This neural network is performing worse than both the dummy regressor, and the current best model, Ridge regression (MAE = 2.58). We will exclude this model from further investigation. \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3864e79",
   "metadata": {},
   "source": [
    "<a id='Step6'></a>\n",
    "## 6. Best model with word embeddings\n",
    "Since we have the word embeddings loaded, let's try fitting Ridge regression with these embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d283d880",
   "metadata": {},
   "source": [
    "### 6.1. Ridge regression with word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e4854f85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lowest MAE is 2.829434692632794 with a regularization term of 1\n",
      "The MAE of the test set is 2.866447672760307\n"
     ]
    }
   ],
   "source": [
    "# Ridge with w2v\n",
    "reg_term_list = list(range(1,31))\n",
    "\n",
    "cv_scores = []\n",
    "best_score = np.inf\n",
    "best_alpha = []\n",
    "\n",
    "for alpha in reg_term_list: \n",
    "    rr = Ridge(alpha=alpha)\n",
    "    cv_score = np.mean(cross_val_score(rr, X_train_w2v, y_train, cv=5, scoring=scoring))*-1\n",
    "\n",
    "    cv_scores.append(cv_score)\n",
    "\n",
    "    if cv_score < best_score:\n",
    "        best_score = cv_score\n",
    "        best_alpha = alpha\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(reg_term_list, cv_scores, color='red', label = 'Cross Validation Score')\n",
    "plt.title('MAE for different values of alpha')\n",
    "plt.xlabel('Alpha value')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f'The lowest MAE is {best_score} with a regularization term of {best_alpha}') \n",
    "\n",
    "# recreate best model and test against test set\n",
    "w2v_rr_best = Ridge(alpha=best_alpha)\n",
    "w2v_rr_best.fit(X_train_w2v, y_train) \n",
    "\n",
    "y_pred = w2v_rr_best.predict(X_test_w2v)\n",
    "w2v_best_rr_clipped_mae = clipped_mae(y_test, y_pred)\n",
    "\n",
    "print(f'The MAE of the test set is {w2v_best_rr_clipped_mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff211f2",
   "metadata": {},
   "source": [
    "### Ridge regression with GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d7a236e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lowest MAE is 2.769781133280677 with a regularization term of 1\n",
      "The MAE of the test set is 2.88721491281618\n"
     ]
    }
   ],
   "source": [
    "# Ridge with GloVe\n",
    "reg_term_list = list(range(1,31))\n",
    "\n",
    "cv_scores = []\n",
    "best_score = np.inf\n",
    "best_alpha = []\n",
    "\n",
    "for alpha in reg_term_list: \n",
    "    rr = Ridge(alpha=alpha)\n",
    "    cv_score = np.mean(cross_val_score(rr, X_train_glove, y_train, cv=5, scoring=scoring))*-1\n",
    "\n",
    "    cv_scores.append(cv_score)\n",
    "\n",
    "    if cv_score < best_score:\n",
    "        best_score = cv_score\n",
    "        best_alpha = alpha\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(reg_term_list, cv_scores, color='red', label = 'Cross Validation Score')\n",
    "plt.title('MAE for different values of alpha')\n",
    "plt.xlabel('Alpha value')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f'The lowest MAE is {best_score} with a regularization term of {best_alpha}') \n",
    "\n",
    "# recreate best model and test against test set\n",
    "glove_rr_best = Ridge(alpha=best_alpha)\n",
    "glove_rr_best.fit(X_train_glove, y_train) \n",
    "\n",
    "y_pred = glove_rr_best.predict(X_test_glove)\n",
    "glove_best_rr_clipped_mae = clipped_mae(y_test, y_pred)\n",
    "\n",
    "print(f'The MAE of the test set is {glove_best_rr_clipped_mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ef6955",
   "metadata": {},
   "source": [
    "The last two models both perform worse than our best model, Ridge regression (MAE = 2.58). We will exclude these neural networks from further investigation. \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ce3acc",
   "metadata": {},
   "source": [
    "# Conclusion \n",
    "In this notebook we used GridSearchCV to optimize the hyperparameters of different models (Ridge regression, Lasso regression, Decision Tree Regressor, KNN Regressor, and XGB Regressor) which helped determine which of these models performed the best. GridSearchCV uses cross validation scores to determine which model performs the best. We also compared how Bag of Words and TF-IDF to see which form of text representation resulted in the best cross validation score. The best models found were Ridge regression with TF-IDF vectorization (MAE=2.58), and XGBRegressor with TF-IDF vectorization (MAE=2.49). \n",
    "\n",
    "While the XGBRegressor model performed the best, it lacks the level of interpretability that we are looking for. We want to see how individual words affect the predicted rating of a whisky, and this is not possible with XGBoost regression models. We could perhaps look at the feature importances of our XGB regression model, but that does not indicate whether a word increases or decreases a whisky's rating. Due to to lack of interpretability, this is not a model we will be investigating further.  \n",
    "\n",
    "We also fit neural networks using TF-IDF vectorization, and word2vec and GlovE word embeddings. These models did not perform as well as Ridge regression and the XGB Regressor and will not be investigated further. Please note that there was no attempt at optimizing the neural network. The main goal of this investigation was to observe the regression coefficients of whisky reviews to determine which words increase and decrease the whisky's rating. This level of interpretability is largely lost when using neural networks. We fit these models mainly out of curiousity to see if they were more accurate than our other models without tuning hyperparameters. \n",
    "\n",
    "Finally, we fit Ridge regression models using word2vec and GloVe word embeddings as well. These models also did not perform as well as Ridge regression and XGB Regressor with TF-IDF vectorization.  \n",
    "\n",
    "In our next notebook, we will further investigate Ridge regression with TF-IDF text vectorization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48c007c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
